

** [alpha] Launch Training on 20230811_1027
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "early_stop": false,
  "lr_scheduling": "exp",
  "lr": 0.0008,
  "stepwise_scheduling": true,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230811_1029 @20230811_1029 [EP 1/500] lr: 0.0007867820095270872, took: 114.80s (avg: 114.80s), ETA: 0d:15h:48m:6s [TRAIN] loss: 0.1518, iou: 0.4487, f1: 0.5650 [EVAL] loss: 0.3218, iou: 0.0001, f1: 0.0002
@20230811_1030 @20230811_1030 [EP 2/500] lr: 0.0007739338907413185, took: 61.55s (avg: 61.55s), ETA: 0d:8h:26m:18s [TRAIN] loss: 0.0678, iou: 0.7245, f1: 0.8386 [EVAL] loss: 0.3229, iou: 0.0000, f1: 0.0000
@20230811_1031 @20230811_1031 [EP 3/500] lr: 0.0007614986388944089, took: 61.60s (avg: 61.60s), ETA: 0d:8h:25m:17s [TRAIN] loss: 0.0574, iou: 0.7744, f1: 0.8721 [EVAL] loss: 0.3229, iou: 0.0000, f1: 0.0000
@20230811_1032 @20230811_1032 [EP 4/500] lr: 0.0007494565797969699, took: 61.79s (avg: 61.79s), ETA: 0d:8h:24m:16s [TRAIN] loss: 0.0531, iou: 0.7968, f1: 0.8862 [EVAL] loss: 0.3133, iou: 0.0169, f1: 0.0312
@20230811_1034 @20230811_1034 [EP 5/500] lr: 0.0007377894944511354, took: 61.66s (avg: 61.66s), ETA: 0d:8h:23m:15s [TRAIN] loss: 0.0509, iou: 0.8080, f1: 0.8932 [EVAL] loss: 0.2173, iou: 0.2163, f1: 0.3464
@20230811_1035 @20230811_1035 [EP 6/500] lr: 0.000726480211596936, took: 61.56s (avg: 61.56s), ETA: 0d:8h:22m:14s [TRAIN] loss: 0.0474, iou: 0.8265, f1: 0.9046 [EVAL] loss: 0.1060, iou: 0.5546, f1: 0.7103
@20230811_1036 @20230811_1036 [EP 7/500] lr: 0.0007155122584663332, took: 61.62s (avg: 61.62s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0457, iou: 0.8359, f1: 0.9103 [EVAL] loss: 0.0635, iou: 0.7426, f1: 0.8509
@20230811_1038 @20230811_1038 [EP 8/500] lr: 0.0007048706174828112, took: 61.76s (avg: 61.76s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0462, iou: 0.8331, f1: 0.9085 [EVAL] loss: 0.0768, iou: 0.6780, f1: 0.8059
@20230811_1039 @20230811_1039 [EP 9/500] lr: 0.0006945409113541245, took: 61.68s (avg: 61.68s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0451, iou: 0.8392, f1: 0.9121 [EVAL] loss: 0.1514, iou: 0.3967, f1: 0.5618
@20230811_1040 @20230811_1040 [EP 10/500] lr: 0.0006845095194876194, took: 61.69s (avg: 61.69s), ETA: 0d:8h:18m:10s [TRAIN] loss: 0.0440, iou: 0.8452, f1: 0.9158 [EVAL] loss: 0.0581, iou: 0.7687, f1: 0.8676
@20230811_1041 @20230811_1041 [EP 11/500] lr: 0.0006747638108208776, took: 61.64s (avg: 61.64s), ETA: 0d:8h:17m:9s [TRAIN] loss: 0.0431, iou: 0.8498, f1: 0.9184 [EVAL] loss: 0.0590, iou: 0.7611, f1: 0.8635
@20230811_1042 @20230811_1042 [EP 12/500] lr: 0.000665291678160429, took: 61.56s (avg: 61.56s), ETA: 0d:8h:16m:8s [TRAIN] loss: 0.0416, iou: 0.8581, f1: 0.9233 [EVAL] loss: 0.0714, iou: 0.7029, f1: 0.8237
@20230811_1043 @20230811_1043 [EP 13/500] lr: 0.0006560818874277174, took: 61.64s (avg: 61.64s), ETA: 0d:8h:15m:7s [TRAIN] loss: 0.0410, iou: 0.8615, f1: 0.9253 [EVAL] loss: 0.1047, iou: 0.5602, f1: 0.7146
@20230811_1044 @20230811_1044 [EP 14/500] lr: 0.0006471234955824912, took: 61.62s (avg: 61.62s), ETA: 0d:8h:14m:6s [TRAIN] loss: 0.0397, iou: 0.8685, f1: 0.9294 [EVAL] loss: 0.0799, iou: 0.6635, f1: 0.7957
@20230811_1045 @20230811_1045 [EP 15/500] lr: 0.0006384065491147339, took: 61.63s (avg: 61.63s), ETA: 0d:8h:13m:5s [TRAIN] loss: 0.0395, iou: 0.8700, f1: 0.9303 [EVAL] loss: 0.0706, iou: 0.7056, f1: 0.8260
@20230811_1046 @20230811_1046 [EP 16/500] lr: 0.0006299212691374123, took: 61.93s (avg: 61.93s), ETA: 0d:8h:12m:4s [TRAIN] loss: 0.0382, iou: 0.8769, f1: 0.9342 [EVAL] loss: 0.0548, iou: 0.7833, f1: 0.8778
@20230811_1048 @20230811_1048 [EP 17/500] lr: 0.0006216585752554238, took: 61.85s (avg: 61.85s), ETA: 0d:8h:11m:3s [TRAIN] loss: 0.0375, iou: 0.8810, f1: 0.9366 [EVAL] loss: 0.0734, iou: 0.6931, f1: 0.8170
@20230811_1049 @20230811_1049 [EP 18/500] lr: 0.0006136098527349532, took: 61.87s (avg: 61.87s), ETA: 0d:8h:10m:2s [TRAIN] loss: 0.0379, iou: 0.8792, f1: 0.9355 [EVAL] loss: 0.0880, iou: 0.6287, f1: 0.7693
@20230811_1050 @20230811_1050 [EP 19/500] lr: 0.0006057668360881507, took: 61.71s (avg: 61.71s), ETA: 0d:8h:9m:1s [TRAIN] loss: 0.0382, iou: 0.8773, f1: 0.9344 [EVAL] loss: 0.1120, iou: 0.5324, f1: 0.6911
@20230811_1051 @20230811_1051 [EP 20/500] lr: 0.0005981219001114368, took: 61.97s (avg: 61.97s), ETA: 0d:8h:8m:0s [TRAIN] loss: 0.0373, iou: 0.8824, f1: 0.9373 [EVAL] loss: 0.0798, iou: 0.6641, f1: 0.7964
@20230811_1052 @20230811_1052 [EP 21/500] lr: 0.0005906674196012318, took: 61.73s (avg: 61.73s), ETA: 0d:8h:6m:59s [TRAIN] loss: 0.0368, iou: 0.8854, f1: 0.9391 [EVAL] loss: 0.0475, iou: 0.8223, f1: 0.9020
@20230811_1053 @20230811_1053 [EP 22/500] lr: 0.0005833965260535479, took: 61.71s (avg: 61.71s), ETA: 0d:8h:5m:58s [TRAIN] loss: 0.0382, iou: 0.8775, f1: 0.9346 [EVAL] loss: 0.0566, iou: 0.7746, f1: 0.8722
@20230811_1054 @20230811_1054 [EP 23/500] lr: 0.0005763024091720581, took: 61.87s (avg: 61.87s), ETA: 0d:8h:4m:57s [TRAIN] loss: 0.0382, iou: 0.8771, f1: 0.9343 [EVAL] loss: 0.0747, iou: 0.6866, f1: 0.8125
@20230811_1055 @20230811_1055 [EP 24/500] lr: 0.0005693787825293839, took: 61.77s (avg: 61.77s), ETA: 0d:8h:3m:56s [TRAIN] loss: 0.0372, iou: 0.8829, f1: 0.9376 [EVAL] loss: 0.0760, iou: 0.6815, f1: 0.8087
@20230811_1056 @20230811_1056 [EP 25/500] lr: 0.0005626195343211293, took: 61.75s (avg: 61.75s), ETA: 0d:8h:2m:55s [TRAIN] loss: 0.0368, iou: 0.8855, f1: 0.9391 [EVAL] loss: 0.0637, iou: 0.7391, f1: 0.8488
@20230811_1057 @20230811_1057 [EP 26/500] lr: 0.000556018843781203, took: 61.78s (avg: 61.78s), ETA: 0d:8h:1m:54s [TRAIN] loss: 0.0361, iou: 0.8891, f1: 0.9411 [EVAL] loss: 0.0777, iou: 0.6723, f1: 0.8027
@20230811_1058 @20230811_1058 [EP 27/500] lr: 0.0005495712975971401, took: 61.69s (avg: 61.69s), ETA: 0d:8h:0m:53s [TRAIN] loss: 0.0353, iou: 0.8939, f1: 0.9438 [EVAL] loss: 0.0677, iou: 0.7192, f1: 0.8357
@20230811_1059 @20230811_1059 [EP 28/500] lr: 0.0005432715406641364, took: 61.65s (avg: 61.65s), ETA: 0d:7h:59m:52s [TRAIN] loss: 0.0345, iou: 0.8985, f1: 0.9464 [EVAL] loss: 0.0587, iou: 0.7630, f1: 0.8648
