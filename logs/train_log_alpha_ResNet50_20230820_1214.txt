

** [alpha] Launch Training on 20230820_1214
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "default",
  "early_stop": false,
  "lr_scheduling": "const",
  "lr": 6e-05,
  "stepwise_scheduling": false,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230820_1216 @20230820_1216 [EP 1/500] lr: 5.999999848427251e-05, took: 114.81s (avg: 114.81s), ETA: 0d:15h:48m:6s [TRAIN] loss: 0.8864, iou: 0.0630, f1: 0.1136 [EVAL] loss: 0.9946, iou: 0.0027, f1: 0.0054
@20230820_1217 @20230820_1217 [EP 2/500] lr: 5.999999848427251e-05, took: 63.00s (avg: 63.00s), ETA: 0d:8h:42m:54s [TRAIN] loss: 0.5285, iou: 0.3204, f1: 0.4715 [EVAL] loss: 0.9975, iou: 0.0012, f1: 0.0025
@20230820_1218 @20230820_1218 [EP 3/500] lr: 5.999999848427251e-05, took: 62.94s (avg: 62.94s), ETA: 0d:8h:33m:34s [TRAIN] loss: 0.2666, iou: 0.5844, f1: 0.7334 [EVAL] loss: 0.9992, iou: 0.0004, f1: 0.0008
@20230820_1219 @20230820_1219 [EP 4/500] lr: 5.999999848427251e-05, took: 63.17s (avg: 63.17s), ETA: 0d:8h:40m:48s [TRAIN] loss: 0.1706, iou: 0.7106, f1: 0.8294 [EVAL] loss: 0.9964, iou: 0.0019, f1: 0.0036
@20230820_1220 @20230820_1220 [EP 5/500] lr: 5.999999848427251e-05, took: 58.36s (avg: 58.36s), ETA: 0d:7h:58m:30s [TRAIN] loss: 0.1355, iou: 0.7626, f1: 0.8645 [EVAL] loss: 0.7128, iou: 0.1729, f1: 0.2872
@20230820_1221 @20230820_1221 [EP 6/500] lr: 5.999999848427251e-05, took: 58.14s (avg: 58.14s), ETA: 0d:7h:57m:32s [TRAIN] loss: 0.1179, iou: 0.7901, f1: 0.8821 [EVAL] loss: 0.2959, iou: 0.5459, f1: 0.7041
@20230820_1222 @20230820_1222 [EP 7/500] lr: 5.999999848427251e-05, took: 58.09s (avg: 58.09s), ETA: 0d:7h:56m:34s [TRAIN] loss: 0.1059, iou: 0.8094, f1: 0.8941 [EVAL] loss: 0.2171, iou: 0.6450, f1: 0.7829
@20230820_1223 @20230820_1223 [EP 8/500] lr: 5.999999848427251e-05, took: 58.40s (avg: 58.40s), ETA: 0d:7h:55m:36s [TRAIN] loss: 0.0996, iou: 0.8196, f1: 0.9004 [EVAL] loss: 0.1717, iou: 0.7085, f1: 0.8283
@20230820_1224 @20230820_1224 [EP 9/500] lr: 5.999999848427251e-05, took: 58.37s (avg: 58.37s), ETA: 0d:7h:54m:38s [TRAIN] loss: 0.0903, iou: 0.8349, f1: 0.9097 [EVAL] loss: 0.1900, iou: 0.6829, f1: 0.8100
@20230820_1225 @20230820_1225 [EP 10/500] lr: 5.999999848427251e-05, took: 58.07s (avg: 58.07s), ETA: 0d:7h:53m:40s [TRAIN] loss: 0.0842, iou: 0.8452, f1: 0.9158 [EVAL] loss: 0.2006, iou: 0.6681, f1: 0.7994
@20230820_1226 @20230820_1226 [EP 11/500] lr: 5.999999848427251e-05, took: 58.09s (avg: 58.09s), ETA: 0d:7h:52m:42s [TRAIN] loss: 0.0869, iou: 0.8406, f1: 0.9131 [EVAL] loss: 0.2880, iou: 0.5569, f1: 0.7120
@20230820_1227 @20230820_1227 [EP 12/500] lr: 5.999999848427251e-05, took: 58.10s (avg: 58.10s), ETA: 0d:7h:51m:44s [TRAIN] loss: 0.0840, iou: 0.8455, f1: 0.9160 [EVAL] loss: 0.1833, iou: 0.6920, f1: 0.8167
@20230820_1228 @20230820_1228 [EP 13/500] lr: 5.999999848427251e-05, took: 58.34s (avg: 58.34s), ETA: 0d:7h:50m:46s [TRAIN] loss: 0.0773, iou: 0.8568, f1: 0.9227 [EVAL] loss: 0.1509, iou: 0.7394, f1: 0.8491
@20230820_1229 @20230820_1229 [EP 14/500] lr: 5.999999848427251e-05, took: 58.38s (avg: 58.38s), ETA: 0d:7h:49m:48s [TRAIN] loss: 0.0728, iou: 0.8646, f1: 0.9272 [EVAL] loss: 0.1394, iou: 0.7567, f1: 0.8606
@20230820_1230 @20230820_1230 [EP 15/500] lr: 5.999999848427251e-05, took: 58.77s (avg: 58.77s), ETA: 0d:7h:48m:50s [TRAIN] loss: 0.0704, iou: 0.8688, f1: 0.9296 [EVAL] loss: 0.1641, iou: 0.7202, f1: 0.8359
@20230820_1231 @20230820_1231 [EP 16/500] lr: 5.999999848427251e-05, took: 57.98s (avg: 57.98s), ETA: 0d:7h:39m:48s [TRAIN] loss: 0.0711, iou: 0.8675, f1: 0.9289 [EVAL] loss: 0.2435, iou: 0.6117, f1: 0.7565
@20230820_1232 @20230820_1232 [EP 17/500] lr: 5.999999848427251e-05, took: 58.15s (avg: 58.15s), ETA: 0d:7h:46m:54s [TRAIN] loss: 0.0727, iou: 0.8648, f1: 0.9273 [EVAL] loss: 0.1767, iou: 0.7018, f1: 0.8233
@20230820_1233 @20230820_1233 [EP 18/500] lr: 5.999999848427251e-05, took: 59.90s (avg: 59.90s), ETA: 0d:7h:53m:58s [TRAIN] loss: 0.0730, iou: 0.8643, f1: 0.9270 [EVAL] loss: 0.2275, iou: 0.6324, f1: 0.7725
@20230820_1234 @20230820_1234 [EP 19/500] lr: 5.999999848427251e-05, took: 63.19s (avg: 63.19s), ETA: 0d:8h:25m:3s [TRAIN] loss: 0.0712, iou: 0.8675, f1: 0.9288 [EVAL] loss: 0.1619, iou: 0.7227, f1: 0.8381
@20230820_1235 @20230820_1235 [EP 20/500] lr: 5.999999848427251e-05, took: 63.78s (avg: 63.78s), ETA: 0d:8h:24m:0s [TRAIN] loss: 0.0669, iou: 0.8749, f1: 0.9331 [EVAL] loss: 0.1589, iou: 0.7274, f1: 0.8411
