

** [alpha] Launch Training on 20230809_1742
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "early_stop": false,
  "lr_scheduling": "exp",
  "lr": 0.001,
  "stepwise_scheduling": true,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2ns (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss + focal loss)\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .5 are used \n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230809_1744 @20230809_1744 [EP 1/500] lr: 0.0009794319048523903, took: 127.43s (avg: 127.43s), ETA: 0d:17h:36m:13s [TRAIN] loss: 0.2084, iou: 0.4936, f1: 0.6141 [EVAL] loss: 0.5158, iou: 0.0000, f1: 0.0001
@20230809_1745 @20230809_1745 [EP 2/500] lr: 0.0009596009040251374, took: 73.37s (avg: 73.37s), ETA: 0d:10h:5m:54s [TRAIN] loss: 0.0916, iou: 0.7328, f1: 0.8446 [EVAL] loss: 0.5161, iou: 0.0000, f1: 0.0000
@20230809_1747 @20230809_1747 [EP 3/500] lr: 0.0009405568707734346, took: 73.76s (avg: 73.76s), ETA: 0d:10h:4m:41s [TRAIN] loss: 0.0784, iou: 0.7723, f1: 0.8707 [EVAL] loss: 0.5163, iou: 0.0000, f1: 0.0000
@20230809_1748 @20230809_1748 [EP 4/500] lr: 0.000922253995668143, took: 72.97s (avg: 72.97s), ETA: 0d:9h:55m:12s [TRAIN] loss: 0.0698, iou: 0.7990, f1: 0.8876 [EVAL] loss: 0.5032, iou: 0.0140, f1: 0.0261
@20230809_1750 @20230809_1750 [EP 5/500] lr: 0.0009046499617397785, took: 71.19s (avg: 71.19s), ETA: 0d:9h:45m:45s [TRAIN] loss: 0.0663, iou: 0.8103, f1: 0.8946 [EVAL] loss: 0.3457, iou: 0.2107, f1: 0.3389
@20230809_1751 @20230809_1751 [EP 6/500] lr: 0.0008877053041942418, took: 61.40s (avg: 61.40s), ETA: 0d:8h:22m:14s [TRAIN] loss: 0.0622, iou: 0.8235, f1: 0.9027 [EVAL] loss: 0.1239, iou: 0.6407, f1: 0.7792
@20230809_1752 @20230809_1752 [EP 7/500] lr: 0.0008713837596587837, took: 61.30s (avg: 61.30s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0604, iou: 0.8295, f1: 0.9063 [EVAL] loss: 0.0971, iou: 0.7153, f1: 0.8325
@20230809_1753 @20230809_1753 [EP 8/500] lr: 0.0008556516258977354, took: 61.29s (avg: 61.29s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0595, iou: 0.8323, f1: 0.9080 [EVAL] loss: 0.1117, iou: 0.6741, f1: 0.8033
@20230809_1754 @20230809_1754 [EP 9/500] lr: 0.0008404774125665426, took: 61.53s (avg: 61.53s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0577, iou: 0.8381, f1: 0.9115 [EVAL] loss: 0.1262, iou: 0.6363, f1: 0.7748
@20230809_1756 @20230809_1756 [EP 10/500] lr: 0.0008258320158347487, took: 67.30s (avg: 67.30s), ETA: 0d:9h:7m:10s [TRAIN] loss: 0.0544, iou: 0.8490, f1: 0.9180 [EVAL] loss: 0.2055, iou: 0.4530, f1: 0.6174
@20230809_1757 @20230809_1757 [EP 11/500] lr: 0.000811688369140029, took: 73.27s (avg: 73.27s), ETA: 0d:9h:54m:57s [TRAIN] loss: 0.0553, iou: 0.8463, f1: 0.9163 [EVAL] loss: 0.0956, iou: 0.7201, f1: 0.8352
@20230809_1758 @20230809_1758 [EP 12/500] lr: 0.0007980209193192422, took: 73.08s (avg: 73.08s), ETA: 0d:9h:53m:44s [TRAIN] loss: 0.0524, iou: 0.8560, f1: 0.9221 [EVAL] loss: 0.1313, iou: 0.6217, f1: 0.7646
@20230809_1800 @20230809_1800 [EP 13/500] lr: 0.0007848062086850405, took: 73.08s (avg: 73.08s), ETA: 0d:9h:52m:31s [TRAIN] loss: 0.0520, iou: 0.8573, f1: 0.9228 [EVAL] loss: 0.0943, iou: 0.7235, f1: 0.8382
@20230809_1801 @20230809_1801 [EP 14/500] lr: 0.0007720219437032938, took: 72.97s (avg: 72.97s), ETA: 0d:9h:43m:12s [TRAIN] loss: 0.0488, iou: 0.8681, f1: 0.9291 [EVAL] loss: 0.1177, iou: 0.6594, f1: 0.7923
@20230809_1802 @20230809_1802 [EP 15/500] lr: 0.0007596475188620389, took: 72.71s (avg: 72.71s), ETA: 0d:9h:42m:0s [TRAIN] loss: 0.0483, iou: 0.8696, f1: 0.9300 [EVAL] loss: 0.1491, iou: 0.5790, f1: 0.7293
@20230809_1804 @20230809_1804 [EP 16/500] lr: 0.0007476636092178524, took: 72.95s (avg: 72.95s), ETA: 0d:9h:40m:48s [TRAIN] loss: 0.0518, iou: 0.8582, f1: 0.9233 [EVAL] loss: 0.0736, iou: 0.7853, f1: 0.8789
@20230809_1805 @20230809_1805 [EP 17/500] lr: 0.0007360518211498857, took: 73.51s (avg: 73.51s), ETA: 0d:9h:47m:39s [TRAIN] loss: 0.0497, iou: 0.8650, f1: 0.9273 [EVAL] loss: 0.2768, iou: 0.3204, f1: 0.4757
@20230809_1806 @20230809_1806 [EP 18/500] lr: 0.0007247953326441348, took: 72.54s (avg: 72.54s), ETA: 0d:9h:38m:24s [TRAIN] loss: 0.0478, iou: 0.8714, f1: 0.9310 [EVAL] loss: 0.0930, iou: 0.7264, f1: 0.8404
