

** [alpha] Launch Training on 20230818_1857
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "early_stop": false,
  "lr_scheduling": "const",
  "lr": 6e-05,
  "stepwise_scheduling": false,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230818_1859 @20230818_1859 [EP 1/500] lr: 5.999999848427251e-05, took: 121.37s (avg: 121.37s), ETA: 0d:16h:46m:19s [TRAIN] loss: 0.5462, iou: 0.1180, f1: 0.1975 [EVAL] loss: 0.6733, iou: 0.0024, f1: 0.0049
@20230818_1900 @20230818_1900 [EP 2/500] lr: 5.999999848427251e-05, took: 70.95s (avg: 70.95s), ETA: 0d:9h:41m:0s [TRAIN] loss: 0.2583, iou: 0.4684, f1: 0.6281 [EVAL] loss: 0.6756, iou: 0.0009, f1: 0.0019
@20230818_1902 @20230818_1902 [EP 3/500] lr: 5.999999848427251e-05, took: 72.05s (avg: 72.05s), ETA: 0d:9h:56m:24s [TRAIN] loss: 0.1407, iou: 0.6757, f1: 0.8044 [EVAL] loss: 0.6767, iou: 0.0003, f1: 0.0005
@20230818_1903 @20230818_1903 [EP 4/500] lr: 5.999999848427251e-05, took: 70.61s (avg: 70.61s), ETA: 0d:9h:38m:40s [TRAIN] loss: 0.1071, iou: 0.7479, f1: 0.8547 [EVAL] loss: 0.6737, iou: 0.0026, f1: 0.0050
@20230818_1904 @20230818_1904 [EP 5/500] lr: 5.999999848427251e-05, took: 71.29s (avg: 71.29s), ETA: 0d:9h:45m:45s [TRAIN] loss: 0.0906, iou: 0.7858, f1: 0.8795 [EVAL] loss: 0.5394, iou: 0.1175, f1: 0.2062
@20230818_1905 @20230818_1905 [EP 6/500] lr: 5.999999848427251e-05, took: 70.87s (avg: 70.87s), ETA: 0d:9h:36m:20s [TRAIN] loss: 0.0830, iou: 0.8042, f1: 0.8909 [EVAL] loss: 0.3206, iou: 0.3685, f1: 0.5342
@20230818_1906 @20230818_1906 [EP 7/500] lr: 5.999999848427251e-05, took: 71.72s (avg: 71.72s), ETA: 0d:9h:43m:23s [TRAIN] loss: 0.0765, iou: 0.8200, f1: 0.9007 [EVAL] loss: 0.1974, iou: 0.5636, f1: 0.7187
@20230818_1908 @20230818_1908 [EP 8/500] lr: 5.999999848427251e-05, took: 68.06s (avg: 68.06s), ETA: 0d:9h:17m:36s [TRAIN] loss: 0.0719, iou: 0.8314, f1: 0.9075 [EVAL] loss: 0.1432, iou: 0.6684, f1: 0.8000
@20230818_1909 @20230818_1909 [EP 9/500] lr: 5.999999848427251e-05, took: 61.27s (avg: 61.27s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0690, iou: 0.8387, f1: 0.9119 [EVAL] loss: 0.1251, iou: 0.7068, f1: 0.8272
@20230818_1910 @20230818_1910 [EP 10/500] lr: 5.999999848427251e-05, took: 60.97s (avg: 60.97s), ETA: 0d:8h:10m:0s [TRAIN] loss: 0.0667, iou: 0.8446, f1: 0.9154 [EVAL] loss: 0.1062, iou: 0.7489, f1: 0.8555
@20230818_1911 @20230818_1911 [EP 11/500] lr: 5.999999848427251e-05, took: 61.05s (avg: 61.05s), ETA: 0d:8h:17m:9s [TRAIN] loss: 0.0666, iou: 0.8449, f1: 0.9156 [EVAL] loss: 0.0872, iou: 0.7935, f1: 0.8842
@20230818_1912 @20230818_1912 [EP 12/500] lr: 5.999999848427251e-05, took: 61.06s (avg: 61.06s), ETA: 0d:8h:16m:8s [TRAIN] loss: 0.0646, iou: 0.8498, f1: 0.9185 [EVAL] loss: 0.0998, iou: 0.7641, f1: 0.8652
@20230818_1913 @20230818_1913 [EP 13/500] lr: 5.999999848427251e-05, took: 61.44s (avg: 61.44s), ETA: 0d:8h:15m:7s [TRAIN] loss: 0.0618, iou: 0.8570, f1: 0.9227 [EVAL] loss: 0.1023, iou: 0.7580, f1: 0.8614
@20230818_1914 @20230818_1914 [EP 14/500] lr: 5.999999848427251e-05, took: 61.15s (avg: 61.15s), ETA: 0d:8h:14m:6s [TRAIN] loss: 0.0588, iou: 0.8648, f1: 0.9273 [EVAL] loss: 0.1229, iou: 0.7119, f1: 0.8304
@20230818_1915 @20230818_1915 [EP 15/500] lr: 5.999999848427251e-05, took: 61.33s (avg: 61.33s), ETA: 0d:8h:13m:5s [TRAIN] loss: 0.0566, iou: 0.8704, f1: 0.9305 [EVAL] loss: 0.1551, iou: 0.6446, f1: 0.7823
@20230818_1916 @20230818_1916 [EP 16/500] lr: 5.999999848427251e-05, took: 61.18s (avg: 61.18s), ETA: 0d:8h:12m:4s [TRAIN] loss: 0.0575, iou: 0.8679, f1: 0.9291 [EVAL] loss: 0.1319, iou: 0.6925, f1: 0.8170
@20230818_1917 @20230818_1917 [EP 17/500] lr: 5.999999848427251e-05, took: 71.08s (avg: 71.08s), ETA: 0d:9h:31m:33s [TRAIN] loss: 0.0589, iou: 0.8644, f1: 0.9271 [EVAL] loss: 0.1239, iou: 0.7097, f1: 0.8290
@20230818_1918 @20230818_1918 [EP 18/500] lr: 5.999999848427251e-05, took: 70.74s (avg: 70.74s), ETA: 0d:9h:22m:20s [TRAIN] loss: 0.0599, iou: 0.8619, f1: 0.9256 [EVAL] loss: 0.0847, iou: 0.7998, f1: 0.8882
@20230818_1920 @20230818_1920 [EP 19/500] lr: 5.999999848427251e-05, took: 72.37s (avg: 72.37s), ETA: 0d:9h:37m:12s [TRAIN] loss: 0.0579, iou: 0.8672, f1: 0.9286 [EVAL] loss: 0.0828, iou: 0.8041, f1: 0.8908
@20230818_1921 @20230818_1921 [EP 20/500] lr: 5.999999848427251e-05, took: 63.57s (avg: 63.57s), ETA: 0d:8h:24m:0s [TRAIN] loss: 0.0538, iou: 0.8778, f1: 0.9348 [EVAL] loss: 0.0954, iou: 0.7740, f1: 0.8718
