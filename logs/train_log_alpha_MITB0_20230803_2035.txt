

** [alpha] Launch Training on 20230803_2035
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "SEGFORMER",
  "baseModel": "MITB0",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 10,
  "early_stop": false,
  "patience": 50,
  "lr_scheduling": "wcos",
  "stepwise_scheduling": true,
  "lr": 0.009,
  "optimizer": "AdamW",
  "decay_steps": 1.0,
  "warmup": 30,
  "train_annotation": "./Annotations/consolidate_train_db_20230711_1637.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230711_1637.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (0711_1637 version) - Updated DB \n* This consolidate DB is updated from 0703_2119 version \n* 4 GPUs are used to train models\n* SEGFORMER Model W/ MIT B2 Pretrained Wegiths\n* Optimize perf: AdamW is used. w/ lr=0.001\n",
  "numGPUs": 4,
  "isCompleted": false,
  "config": "./Config/TRAIN_SEGFORMER_NEWDB_20230711_lr.yaml",
  "numTrainDB": 11530,
  "numEvalDB": 3524
}
@20230803_2037 @20230803_2037 [EP 1/500] lr: 0.0002999999560415745, took: 110.51s (avg: 110.51s), ETA: 0d:15h:14m:50s [TRAIN] loss: 0.8991, iou: 0.0559, f1: 0.1009 [EVAL] loss: 0.7426, iou: 0.1503, f1: 0.2574
@20230803_2038 @20230803_2038 [EP 2/500] lr: 0.000599999912083149, took: 43.69s (avg: 43.69s), ETA: 0d:5h:56m:54s [TRAIN] loss: 0.7011, iou: 0.1806, f1: 0.2989 [EVAL] loss: 0.6235, iou: 0.2361, f1: 0.3765
@20230803_2038 @20230803_2038 [EP 3/500] lr: 0.0008999999263323843, took: 43.44s (avg: 43.44s), ETA: 0d:5h:56m:11s [TRAIN] loss: 0.6244, iou: 0.2372, f1: 0.3756 [EVAL] loss: 0.5814, iou: 0.2687, f1: 0.4186
@20230803_2039 @20230803_2039 [EP 4/500] lr: 0.001199999824166298, took: 42.80s (avg: 42.80s), ETA: 0d:5h:47m:12s [TRAIN] loss: 0.5722, iou: 0.2796, f1: 0.4278 [EVAL] loss: 0.5717, iou: 0.2763, f1: 0.4283
@20230803_2040 @20230803_2040 [EP 5/500] lr: 0.0014999998966231942, took: 42.93s (avg: 42.93s), ETA: 0d:5h:46m:30s [TRAIN] loss: 0.5722, iou: 0.2800, f1: 0.4278 [EVAL] loss: 0.5179, iou: 0.3229, f1: 0.4821
@20230803_2041 @20230803_2041 [EP 6/500] lr: 0.0017999998526647687, took: 43.46s (avg: 43.46s), ETA: 0d:5h:54m:2s [TRAIN] loss: 0.5399, iou: 0.3082, f1: 0.4601 [EVAL] loss: 0.4684, iou: 0.3686, f1: 0.5316
@20230803_2041 @20230803_2041 [EP 7/500] lr: 0.0020999996922910213, took: 43.00s (avg: 43.00s), ETA: 0d:5h:53m:19s [TRAIN] loss: 0.4996, iou: 0.3444, f1: 0.5004 [EVAL] loss: 0.4647, iou: 0.3713, f1: 0.5353
@20230803_2042 @20230803_2042 [EP 8/500] lr: 0.002399999648332596, took: 44.29s (avg: 44.29s), ETA: 0d:6h:0m:48s [TRAIN] loss: 0.4869, iou: 0.3564, f1: 0.5131 [EVAL] loss: 0.4454, iou: 0.3899, f1: 0.5546
@20230803_2043 @20230803_2043 [EP 9/500] lr: 0.002699999837204814, took: 43.35s (avg: 43.35s), ETA: 0d:5h:51m:53s [TRAIN] loss: 0.4815, iou: 0.3614, f1: 0.5185 [EVAL] loss: 0.4508, iou: 0.3849, f1: 0.5492
@20230803_2044 @20230803_2044 [EP 10/500] lr: 0.0029999997932463884, took: 43.67s (avg: 43.67s), ETA: 0d:5h:51m:10s [TRAIN] loss: 0.5075, iou: 0.3379, f1: 0.4925 [EVAL] loss: 0.4476, iou: 0.3884, f1: 0.5524
@20230803_2044 @20230803_2044 [EP 11/500] lr: 0.003299999749287963, took: 37.90s (avg: 37.90s), ETA: 0d:5h:1m:33s [TRAIN] loss: 0.4809, iou: 0.3622, f1: 0.5191 [EVAL] loss: 0.4363, iou: 0.3993, f1: 0.5637
@20230803_2045 @20230803_2045 [EP 12/500] lr: 0.0035999997053295374, took: 37.63s (avg: 37.63s), ETA: 0d:5h:0m:56s [TRAIN] loss: 0.4732, iou: 0.3700, f1: 0.5268 [EVAL] loss: 0.4105, iou: 0.4260, f1: 0.5895
@20230803_2046 @20230803_2046 [EP 13/500] lr: 0.003899999661371112, took: 37.61s (avg: 37.61s), ETA: 0d:5h:0m:19s [TRAIN] loss: 0.4722, iou: 0.3708, f1: 0.5278 [EVAL] loss: 0.4212, iou: 0.4151, f1: 0.5788
