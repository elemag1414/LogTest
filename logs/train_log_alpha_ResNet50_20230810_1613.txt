

** [alpha] Launch Training on 20230810_1613
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "early_stop": false,
  "lr_scheduling": "exp",
  "lr": 0.001,
  "stepwise_scheduling": true,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2ns (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .5 are used \n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230810_1615 @20230810_1615 [EP 1/500] lr: 0.0009794319048523903, took: 115.74s (avg: 115.74s), ETA: 0d:15h:56m:25s [TRAIN] loss: 0.2534, iou: 0.5266, f1: 0.6515 [EVAL] loss: 0.7093, iou: 0.0001, f1: 0.0001
@20230810_1616 @20230810_1616 [EP 2/500] lr: 0.0009596009040251374, took: 61.47s (avg: 61.47s), ETA: 0d:8h:26m:18s [TRAIN] loss: 0.1214, iou: 0.7245, f1: 0.8387 [EVAL] loss: 0.7098, iou: 0.0000, f1: 0.0000
@20230810_1617 @20230810_1617 [EP 3/500] lr: 0.0009405568707734346, took: 61.57s (avg: 61.57s), ETA: 0d:8h:25m:17s [TRAIN] loss: 0.0997, iou: 0.7706, f1: 0.8694 [EVAL] loss: 0.7098, iou: 0.0000, f1: 0.0000
@20230810_1618 @20230810_1618 [EP 4/500] lr: 0.000922253995668143, took: 61.68s (avg: 61.68s), ETA: 0d:8h:24m:16s [TRAIN] loss: 0.0893, iou: 0.7937, f1: 0.8842 [EVAL] loss: 0.6761, iou: 0.0269, f1: 0.0481
@20230810_1619 @20230810_1619 [EP 5/500] lr: 0.0009046499617397785, took: 61.60s (avg: 61.60s), ETA: 0d:8h:23m:15s [TRAIN] loss: 0.0832, iou: 0.8074, f1: 0.8928 [EVAL] loss: 0.4226, iou: 0.2636, f1: 0.4093
@20230810_1621 @20230810_1621 [EP 6/500] lr: 0.0008877053041942418, took: 61.51s (avg: 61.51s), ETA: 0d:8h:22m:14s [TRAIN] loss: 0.0784, iou: 0.8185, f1: 0.8997 [EVAL] loss: 0.1334, iou: 0.7007, f1: 0.8216
@20230810_1622 @20230810_1622 [EP 7/500] lr: 0.0008713837596587837, took: 61.54s (avg: 61.54s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0737, iou: 0.8295, f1: 0.9064 [EVAL] loss: 0.1737, iou: 0.6255, f1: 0.7649
@20230810_1623 @20230810_1623 [EP 8/500] lr: 0.0008556516258977354, took: 61.55s (avg: 61.55s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0677, iou: 0.8437, f1: 0.9148 [EVAL] loss: 0.4913, iou: 0.2574, f1: 0.3699
@20230810_1624 @20230810_1624 [EP 9/500] lr: 0.0008404774125665426, took: 61.65s (avg: 61.65s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0660, iou: 0.8479, f1: 0.9173 [EVAL] loss: 0.1106, iou: 0.7563, f1: 0.8540
@20230810_1625 @20230810_1625 [EP 10/500] lr: 0.0008258320158347487, took: 61.64s (avg: 61.64s), ETA: 0d:8h:18m:10s [TRAIN] loss: 0.0659, iou: 0.8480, f1: 0.9174 [EVAL] loss: 0.1134, iou: 0.7458, f1: 0.8508
@20230810_1626 @20230810_1626 [EP 11/500] lr: 0.000811688369140029, took: 61.45s (avg: 61.45s), ETA: 0d:8h:17m:9s [TRAIN] loss: 0.0663, iou: 0.8472, f1: 0.9169 [EVAL] loss: 0.4835, iou: 0.2374, f1: 0.3435
@20230810_1627 @20230810_1627 [EP 12/500] lr: 0.0007980209193192422, took: 61.53s (avg: 61.53s), ETA: 0d:8h:16m:8s [TRAIN] loss: 0.0645, iou: 0.8515, f1: 0.9194 [EVAL] loss: 0.3273, iou: 0.4688, f1: 0.5802
@20230810_1628 @20230810_1628 [EP 13/500] lr: 0.0007848062086850405, took: 61.54s (avg: 61.54s), ETA: 0d:8h:15m:7s [TRAIN] loss: 0.0628, iou: 0.8557, f1: 0.9219 [EVAL] loss: 0.1395, iou: 0.6918, f1: 0.8133
@20230810_1630 @20230810_1630 [EP 14/500] lr: 0.0007720219437032938, took: 61.52s (avg: 61.52s), ETA: 0d:8h:14m:6s [TRAIN] loss: 0.0613, iou: 0.8593, f1: 0.9240 [EVAL] loss: 0.1025, iou: 0.7707, f1: 0.8660
@20230810_1631 @20230810_1631 [EP 15/500] lr: 0.0007596475188620389, took: 61.58s (avg: 61.58s), ETA: 0d:8h:13m:5s [TRAIN] loss: 0.0581, iou: 0.8671, f1: 0.9285 [EVAL] loss: 0.1287, iou: 0.7105, f1: 0.8279
@20230810_1632 @20230810_1632 [EP 16/500] lr: 0.0007476636092178524, took: 61.53s (avg: 61.53s), ETA: 0d:8h:12m:4s [TRAIN] loss: 0.0579, iou: 0.8675, f1: 0.9288 [EVAL] loss: 0.2351, iou: 0.5166, f1: 0.6763
@20230810_1633 @20230810_1633 [EP 17/500] lr: 0.0007360518211498857, took: 61.49s (avg: 61.49s), ETA: 0d:8h:11m:3s [TRAIN] loss: 0.0560, iou: 0.8723, f1: 0.9315 [EVAL] loss: 0.1339, iou: 0.7011, f1: 0.8208
@20230810_1634 @20230810_1634 [EP 18/500] lr: 0.0007247953326441348, took: 61.56s (avg: 61.56s), ETA: 0d:8h:10m:2s [TRAIN] loss: 0.0575, iou: 0.8685, f1: 0.9293 [EVAL] loss: 0.0992, iou: 0.7759, f1: 0.8708
@20230810_1635 @20230810_1635 [EP 19/500] lr: 0.0007138778455555439, took: 61.86s (avg: 61.86s), ETA: 0d:8h:9m:1s [TRAIN] loss: 0.0562, iou: 0.8716, f1: 0.9312 [EVAL] loss: 0.1235, iou: 0.7243, f1: 0.8364
@20230810_1636 @20230810_1636 [EP 20/500] lr: 0.0007032843423075974, took: 61.94s (avg: 61.94s), ETA: 0d:8h:8m:0s [TRAIN] loss: 0.0540, iou: 0.8771, f1: 0.9343 [EVAL] loss: 0.2912, iou: 0.5259, f1: 0.6307
@20230810_1637 @20230810_1637 [EP 21/500] lr: 0.0006930006784386933, took: 61.69s (avg: 61.69s), ETA: 0d:8h:6m:59s [TRAIN] loss: 0.0517, iou: 0.8828, f1: 0.9375 [EVAL] loss: 0.0979, iou: 0.7756, f1: 0.8718
@20230810_1639 @20230810_1639 [EP 22/500] lr: 0.0006830134661868215, took: 61.80s (avg: 61.80s), ETA: 0d:8h:5m:58s [TRAIN] loss: 0.0516, iou: 0.8831, f1: 0.9377 [EVAL] loss: 0.0939, iou: 0.7841, f1: 0.8774
@20230810_1640 @20230810_1640 [EP 23/500] lr: 0.0006733100162819028, took: 61.67s (avg: 61.67s), ETA: 0d:8h:4m:57s [TRAIN] loss: 0.0516, iou: 0.8831, f1: 0.9377 [EVAL] loss: 0.3088, iou: 0.4939, f1: 0.6058
@20230810_1641 @20230810_1641 [EP 24/500] lr: 0.00066387839615345, took: 61.83s (avg: 61.83s), ETA: 0d:8h:3m:56s [TRAIN] loss: 0.0511, iou: 0.8844, f1: 0.9384 [EVAL] loss: 0.1846, iou: 0.6568, f1: 0.7531
@20230810_1642 @20230810_1642 [EP 25/500] lr: 0.0006547073717229068, took: 61.57s (avg: 61.57s), ETA: 0d:8h:2m:55s [TRAIN] loss: 0.0525, iou: 0.8811, f1: 0.9365 [EVAL] loss: 0.5327, iou: 0.1500, f1: 0.2559
@20230810_1643 @20230810_1643 [EP 26/500] lr: 0.0006457862327806652, took: 61.66s (avg: 61.66s), ETA: 0d:8h:1m:54s [TRAIN] loss: 0.0562, iou: 0.8724, f1: 0.9313 [EVAL] loss: 0.4938, iou: 0.2233, f1: 0.3451
@20230810_1644 @20230810_1644 [EP 27/500] lr: 0.0006371049676090479, took: 61.59s (avg: 61.59s), ETA: 0d:8h:0m:53s [TRAIN] loss: 0.0492, iou: 0.8892, f1: 0.9412 [EVAL] loss: 0.1215, iou: 0.7373, f1: 0.8389
@20230810_1645 @20230810_1645 [EP 28/500] lr: 0.0006286540883593261, took: 61.67s (avg: 61.67s), ETA: 0d:7h:59m:52s [TRAIN] loss: 0.0489, iou: 0.8898, f1: 0.9415 [EVAL] loss: 0.5672, iou: 0.1466, f1: 0.2455
