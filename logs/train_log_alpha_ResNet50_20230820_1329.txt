

** [alpha] Launch Training on 20230820_1329
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "default",
  "early_stop": false,
  "lr_scheduling": "const",
  "lr": 2e-05,
  "stepwise_scheduling": false,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230820_1331 @20230820_1331 [EP 1/500] lr: 1.9999999494757503e-05, took: 114.98s (avg: 114.98s), ETA: 0d:15h:48m:6s [TRAIN] loss: 0.9444, iou: 0.0293, f1: 0.0556 [EVAL] loss: 0.9951, iou: 0.0025, f1: 0.0049
@20230820_1332 @20230820_1332 [EP 2/500] lr: 1.9999999494757503e-05, took: 63.09s (avg: 63.09s), ETA: 0d:8h:42m:54s [TRAIN] loss: 0.8244, iou: 0.0989, f1: 0.1756 [EVAL] loss: 0.9949, iou: 0.0026, f1: 0.0051
@20230820_1333 @20230820_1333 [EP 3/500] lr: 1.9999999494757503e-05, took: 62.46s (avg: 62.46s), ETA: 0d:8h:33m:34s [TRAIN] loss: 0.6865, iou: 0.1911, f1: 0.3135 [EVAL] loss: 0.9958, iou: 0.0021, f1: 0.0042
@20230820_1334 @20230820_1334 [EP 4/500] lr: 1.9999999494757503e-05, took: 62.92s (avg: 62.92s), ETA: 0d:8h:32m:32s [TRAIN] loss: 0.5376, iou: 0.3075, f1: 0.4624 [EVAL] loss: 0.9916, iou: 0.0043, f1: 0.0084
@20230820_1335 @20230820_1335 [EP 5/500] lr: 1.9999999494757503e-05, took: 62.81s (avg: 62.81s), ETA: 0d:8h:31m:30s [TRAIN] loss: 0.4118, iou: 0.4230, f1: 0.5882 [EVAL] loss: 0.7764, iou: 0.1287, f1: 0.2236
@20230820_1336 @20230820_1336 [EP 6/500] lr: 1.9999999494757503e-05, took: 63.51s (avg: 63.51s), ETA: 0d:8h:38m:42s [TRAIN] loss: 0.3209, iou: 0.5192, f1: 0.6791 [EVAL] loss: 0.4916, iou: 0.3440, f1: 0.5084
@20230820_1337 @20230820_1337 [EP 7/500] lr: 1.9999999494757503e-05, took: 63.18s (avg: 63.18s), ETA: 0d:8h:37m:39s [TRAIN] loss: 0.2586, iou: 0.5931, f1: 0.7414 [EVAL] loss: 0.4573, iou: 0.3756, f1: 0.5427
@20230820_1338 @20230820_1338 [EP 8/500] lr: 1.9999999494757503e-05, took: 62.98s (avg: 62.98s), ETA: 0d:8h:28m:24s [TRAIN] loss: 0.2145, iou: 0.6498, f1: 0.7855 [EVAL] loss: 0.5467, iou: 0.2968, f1: 0.4533
@20230820_1340 @20230820_1340 [EP 9/500] lr: 1.9999999494757503e-05, took: 64.23s (avg: 64.23s), ETA: 0d:8h:43m:44s [TRAIN] loss: 0.1823, iou: 0.6938, f1: 0.8177 [EVAL] loss: 0.2581, iou: 0.5932, f1: 0.7419
@20230820_1341 @20230820_1341 [EP 10/500] lr: 1.9999999494757503e-05, took: 62.76s (avg: 62.76s), ETA: 0d:8h:26m:20s [TRAIN] loss: 0.1634, iou: 0.7211, f1: 0.8366 [EVAL] loss: 0.2487, iou: 0.6066, f1: 0.7513
@20230820_1342 @20230820_1342 [EP 11/500] lr: 1.9999999494757503e-05, took: 57.95s (avg: 57.95s), ETA: 0d:7h:44m:33s [TRAIN] loss: 0.1415, iou: 0.7535, f1: 0.8585 [EVAL] loss: 0.2181, iou: 0.6484, f1: 0.7819
@20230820_1343 @20230820_1343 [EP 12/500] lr: 1.9999999494757503e-05, took: 57.95s (avg: 57.95s), ETA: 0d:7h:43m:36s [TRAIN] loss: 0.1263, iou: 0.7768, f1: 0.8737 [EVAL] loss: 0.2071, iou: 0.6625, f1: 0.7929
@20230820_1344 @20230820_1344 [EP 13/500] lr: 1.9999999494757503e-05, took: 57.95s (avg: 57.95s), ETA: 0d:7h:42m:39s [TRAIN] loss: 0.1156, iou: 0.7937, f1: 0.8844 [EVAL] loss: 0.2298, iou: 0.6330, f1: 0.7702
