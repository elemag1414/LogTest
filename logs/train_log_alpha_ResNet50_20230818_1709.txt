

** [alpha] Launch Training on 20230818_1709
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "early_stop": false,
  "lr_scheduling": "const",
  "lr": 6e-05,
  "stepwise_scheduling": false,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230818_1711 @20230818_1711 [EP 1/500] lr: 5.999999848427251e-05, took: 117.67s (avg: 117.67s), ETA: 0d:16h:13m:3s [TRAIN] loss: 0.5528, iou: 0.0433, f1: 0.0806 [EVAL] loss: 0.5893, iou: 0.0026, f1: 0.0052
@20230818_1712 @20230818_1712 [EP 2/500] lr: 5.999999848427251e-05, took: 66.11s (avg: 66.11s), ETA: 0d:9h:7m:48s [TRAIN] loss: 0.3744, iou: 0.2413, f1: 0.3747 [EVAL] loss: 0.5890, iou: 0.0017, f1: 0.0035
@20230818_1713 @20230818_1713 [EP 3/500] lr: 5.999999848427251e-05, took: 66.34s (avg: 66.34s), ETA: 0d:9h:6m:42s [TRAIN] loss: 0.1871, iou: 0.5418, f1: 0.6970 [EVAL] loss: 0.5908, iou: 0.0006, f1: 0.0013
@20230818_1715 @20230818_1715 [EP 4/500] lr: 5.999999848427251e-05, took: 65.43s (avg: 65.43s), ETA: 0d:8h:57m:20s [TRAIN] loss: 0.1166, iou: 0.6952, f1: 0.8186 [EVAL] loss: 0.5859, iou: 0.0052, f1: 0.0100
@20230818_1716 @20230818_1716 [EP 5/500] lr: 5.999999848427251e-05, took: 61.20s (avg: 61.20s), ETA: 0d:8h:23m:15s [TRAIN] loss: 0.0929, iou: 0.7549, f1: 0.8595 [EVAL] loss: 0.4167, iou: 0.1819, f1: 0.3014
@20230818_1717 @20230818_1717 [EP 6/500] lr: 5.999999848427251e-05, took: 60.86s (avg: 60.86s), ETA: 0d:8h:14m:0s [TRAIN] loss: 0.0800, iou: 0.7892, f1: 0.8816 [EVAL] loss: 0.2332, iou: 0.4501, f1: 0.6173
@20230818_1718 @20230818_1718 [EP 7/500] lr: 5.999999848427251e-05, took: 61.08s (avg: 61.08s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0762, iou: 0.8000, f1: 0.8882 [EVAL] loss: 0.4841, iou: 0.1810, f1: 0.2767
@20230818_1719 @20230818_1719 [EP 8/500] lr: 5.999999848427251e-05, took: 60.87s (avg: 60.87s), ETA: 0d:8h:12m:0s [TRAIN] loss: 0.0716, iou: 0.8127, f1: 0.8961 [EVAL] loss: 0.3064, iou: 0.4401, f1: 0.5602
@20230818_1720 @20230818_1720 [EP 9/500] lr: 5.999999848427251e-05, took: 61.09s (avg: 61.09s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0658, iou: 0.8290, f1: 0.9061 [EVAL] loss: 0.5913, iou: 0.2202, f1: 0.2761
@20230818_1721 @20230818_1721 [EP 10/500] lr: 5.999999848427251e-05, took: 60.96s (avg: 60.96s), ETA: 0d:8h:10m:0s [TRAIN] loss: 0.0618, iou: 0.8406, f1: 0.9131 [EVAL] loss: 0.2100, iou: 0.5903, f1: 0.6821
@20230818_1722 @20230818_1722 [EP 11/500] lr: 5.999999848427251e-05, took: 60.89s (avg: 60.89s), ETA: 0d:8h:9m:0s [TRAIN] loss: 0.0600, iou: 0.8458, f1: 0.9162 [EVAL] loss: 0.1750, iou: 0.6052, f1: 0.7219
@20230818_1723 @20230818_1723 [EP 12/500] lr: 5.999999848427251e-05, took: 60.97s (avg: 60.97s), ETA: 0d:8h:8m:0s [TRAIN] loss: 0.0617, iou: 0.8411, f1: 0.9133 [EVAL] loss: 0.1630, iou: 0.6496, f1: 0.7427
@20230818_1724 @20230818_1724 [EP 13/500] lr: 5.999999848427251e-05, took: 60.91s (avg: 60.91s), ETA: 0d:8h:7m:0s [TRAIN] loss: 0.0592, iou: 0.8482, f1: 0.9176 [EVAL] loss: 0.3011, iou: 0.4896, f1: 0.5763
@20230818_1725 @20230818_1725 [EP 14/500] lr: 5.999999848427251e-05, took: 61.53s (avg: 61.53s), ETA: 0d:8h:14m:6s [TRAIN] loss: 0.0570, iou: 0.8547, f1: 0.9214 [EVAL] loss: 0.4733, iou: 0.3113, f1: 0.3924
@20230818_1726 @20230818_1726 [EP 15/500] lr: 5.999999848427251e-05, took: 61.20s (avg: 61.20s), ETA: 0d:8h:13m:5s [TRAIN] loss: 0.0540, iou: 0.8634, f1: 0.9265 [EVAL] loss: 0.1362, iou: 0.6510, f1: 0.7843
@20230818_1727 @20230818_1727 [EP 16/500] lr: 5.999999848427251e-05, took: 61.02s (avg: 61.02s), ETA: 0d:8h:12m:4s [TRAIN] loss: 0.0534, iou: 0.8651, f1: 0.9275 [EVAL] loss: 0.1709, iou: 0.6103, f1: 0.7293
@20230818_1728 @20230818_1728 [EP 17/500] lr: 5.999999848427251e-05, took: 61.18s (avg: 61.18s), ETA: 0d:8h:11m:3s [TRAIN] loss: 0.0539, iou: 0.8639, f1: 0.9268 [EVAL] loss: 0.3034, iou: 0.5317, f1: 0.6108
@20230818_1729 @20230818_1729 [EP 18/500] lr: 5.999999848427251e-05, took: 61.01s (avg: 61.01s), ETA: 0d:8h:10m:2s [TRAIN] loss: 0.0536, iou: 0.8648, f1: 0.9273 [EVAL] loss: 0.3935, iou: 0.4012, f1: 0.4797
@20230818_1730 @20230818_1730 [EP 19/500] lr: 5.999999848427251e-05, took: 60.90s (avg: 60.90s), ETA: 0d:8h:1m:0s [TRAIN] loss: 0.0516, iou: 0.8705, f1: 0.9306 [EVAL] loss: 0.2348, iou: 0.6097, f1: 0.6888
@20230818_1731 @20230818_1731 [EP 20/500] lr: 5.999999848427251e-05, took: 60.92s (avg: 60.92s), ETA: 0d:8h:0m:0s [TRAIN] loss: 0.0497, iou: 0.8763, f1: 0.9339 [EVAL] loss: 0.2662, iou: 0.5754, f1: 0.6626
@20230818_1732 @20230818_1732 [EP 21/500] lr: 5.999999848427251e-05, took: 61.12s (avg: 61.12s), ETA: 0d:8h:6m:59s [TRAIN] loss: 0.0503, iou: 0.8745, f1: 0.9329 [EVAL] loss: 0.2526, iou: 0.5786, f1: 0.6688
@20230818_1733 @20230818_1733 [EP 22/500] lr: 5.999999848427251e-05, took: 60.99s (avg: 60.99s), ETA: 0d:7h:58m:0s [TRAIN] loss: 0.0517, iou: 0.8703, f1: 0.9305 [EVAL] loss: 0.2414, iou: 0.6157, f1: 0.6964
@20230818_1734 @20230818_1734 [EP 23/500] lr: 5.999999848427251e-05, took: 60.87s (avg: 60.87s), ETA: 0d:7h:57m:0s [TRAIN] loss: 0.0515, iou: 0.8710, f1: 0.9309 [EVAL] loss: 0.1674, iou: 0.6802, f1: 0.7621
@20230818_1735 @20230818_1735 [EP 24/500] lr: 5.999999848427251e-05, took: 60.88s (avg: 60.88s), ETA: 0d:7h:56m:0s [TRAIN] loss: 0.0506, iou: 0.8736, f1: 0.9324 [EVAL] loss: 0.0769, iou: 0.7973, f1: 0.8866
@20230818_1736 @20230818_1736 [EP 25/500] lr: 5.999999848427251e-05, took: 60.94s (avg: 60.94s), ETA: 0d:7h:55m:0s [TRAIN] loss: 0.0494, iou: 0.8774, f1: 0.9345 [EVAL] loss: 0.0823, iou: 0.7825, f1: 0.8772
@20230818_1737 @20230818_1737 [EP 26/500] lr: 5.999999848427251e-05, took: 61.12s (avg: 61.12s), ETA: 0d:8h:1m:54s [TRAIN] loss: 0.0492, iou: 0.8780, f1: 0.9349 [EVAL] loss: 0.0915, iou: 0.7577, f1: 0.8613
