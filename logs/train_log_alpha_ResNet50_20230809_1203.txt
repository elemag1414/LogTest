

** [alpha] Launch Training on 20230809_1203
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "early_stop": false,
  "lr_scheduling": "exp",
  "lr": 0.001,
  "stepwise_scheduling": true,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2ns (0809_1202):    lr  0.001 \n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230809_1205 @20230809_1205 [EP 1/500] lr: 0.0009794319048523903, took: 120.65s (avg: 120.65s), ETA: 0d:16h:38m:0s [TRAIN] loss: 0.3551, iou: 0.5145, f1: 0.6449 [EVAL] loss: 1.0000, iou: 0.0000, f1: 0.0000
@20230809_1206 @20230809_1206 [EP 2/500] lr: 0.0009596009040251374, took: 61.72s (avg: 61.72s), ETA: 0d:8h:26m:18s [TRAIN] loss: 0.1575, iou: 0.7298, f1: 0.8425 [EVAL] loss: 1.0000, iou: 0.0000, f1: 0.0000
@20230809_1207 @20230809_1207 [EP 3/500] lr: 0.0009405568707734346, took: 58.66s (avg: 58.66s), ETA: 0d:8h:0m:26s [TRAIN] loss: 0.1324, iou: 0.7678, f1: 0.8676 [EVAL] loss: 1.0000, iou: 0.0000, f1: 0.0000
@20230809_1208 @20230809_1208 [EP 4/500] lr: 0.000922253995668143, took: 58.76s (avg: 58.76s), ETA: 0d:7h:59m:28s [TRAIN] loss: 0.1132, iou: 0.7975, f1: 0.8868 [EVAL] loss: 0.9728, iou: 0.0149, f1: 0.0272
@20230809_1209 @20230809_1209 [EP 5/500] lr: 0.0009046499617397785, took: 59.90s (avg: 59.90s), ETA: 0d:8h:6m:45s [TRAIN] loss: 0.1047, iou: 0.8114, f1: 0.8953 [EVAL] loss: 0.6260, iou: 0.2395, f1: 0.3740
@20230809_1211 @20230809_1211 [EP 6/500] lr: 0.0008877053041942418, took: 58.76s (avg: 58.76s), ETA: 0d:7h:57m:32s [TRAIN] loss: 0.0973, iou: 0.8233, f1: 0.9027 [EVAL] loss: 0.2109, iou: 0.6542, f1: 0.7891
@20230809_1212 @20230809_1212 [EP 7/500] lr: 0.0008713837596587837, took: 58.91s (avg: 58.91s), ETA: 0d:7h:56m:34s [TRAIN] loss: 0.0941, iou: 0.8287, f1: 0.9059 [EVAL] loss: 0.3072, iou: 0.5350, f1: 0.6928
@20230809_1213 @20230809_1213 [EP 8/500] lr: 0.0008556516258977354, took: 58.81s (avg: 58.81s), ETA: 0d:7h:55m:36s [TRAIN] loss: 0.0918, iou: 0.8326, f1: 0.9082 [EVAL] loss: 0.3404, iou: 0.4997, f1: 0.6596
