

** [alpha] Launch Training on 20230803_1920
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "SEGFORMER",
  "baseModel": "MITB0",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 10,
  "early_stop": false,
  "patience": 50,
  "lr_scheduling": "wcos",
  "stepwise_scheduling": true,
  "lr": 0.0001,
  "optimizer": "AdamW",
  "decay_steps": 1.0,
  "warmup": 50,
  "train_annotation": "./Annotations/consolidate_train_db_20230711_1637.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230711_1637.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (0711_1637 version) - Updated DB \n* This consolidate DB is updated from 0703_2119 version \n* 4 GPUs are used to train models\n* SEGFORMER Model W/ MIT B2 Pretrained Wegiths\n* Optimize perf: AdamW is used. w/ lr=0.001\n",
  "numGPUs": 4,
  "isCompleted": false,
  "config": "./Config/TRAIN_SEGFORMER_NEWDB_20230711_lr.yaml",
  "numTrainDB": 11530,
  "numEvalDB": 3524
}
@20230803_1922 @20230803_1922 [EP 1/500] lr: 1.9999999949504854e-06, took: 111.73s (avg: 111.73s), ETA: 0d:15h:23m:9s [TRAIN] loss: 0.9926, iou: 0.0037, f1: 0.0074 [EVAL] loss: 0.9906, iou: 0.0047, f1: 0.0094
@20230803_1923 @20230803_1923 [EP 2/500] lr: 3.999999989900971e-06, took: 42.80s (avg: 42.80s), ETA: 0d:5h:48m:36s [TRAIN] loss: 0.9879, iou: 0.0061, f1: 0.0121 [EVAL] loss: 0.9856, iou: 0.0073, f1: 0.0144
@20230803_1924 @20230803_1924 [EP 3/500] lr: 6.000000212225132e-06, took: 42.05s (avg: 42.05s), ETA: 0d:5h:47m:54s [TRAIN] loss: 0.9770, iou: 0.0118, f1: 0.0230 [EVAL] loss: 0.9658, iou: 0.0174, f1: 0.0342
@20230803_1925 @20230803_1925 [EP 4/500] lr: 7.999999979801942e-06, took: 43.14s (avg: 43.14s), ETA: 0d:5h:55m:28s [TRAIN] loss: 0.9521, iou: 0.0249, f1: 0.0479 [EVAL] loss: 0.9309, iou: 0.0360, f1: 0.0691
@20230803_1925 @20230803_1925 [EP 5/500] lr: 9.999999747378752e-06, took: 42.89s (avg: 42.89s), ETA: 0d:5h:46m:30s [TRAIN] loss: 0.9078, iou: 0.0495, f1: 0.0922 [EVAL] loss: 0.8608, iou: 0.0756, f1: 0.1392
@20230803_1926 @20230803_1926 [EP 6/500] lr: 1.2000000424450263e-05, took: 42.33s (avg: 42.33s), ETA: 0d:5h:45m:48s [TRAIN] loss: 0.8560, iou: 0.0795, f1: 0.1440 [EVAL] loss: 0.8066, iou: 0.1082, f1: 0.1934
@20230803_1927 @20230803_1927 [EP 7/500] lr: 1.4000000192027073e-05, took: 43.24s (avg: 43.24s), ETA: 0d:5h:53m:19s [TRAIN] loss: 0.8150, iou: 0.1044, f1: 0.1850 [EVAL] loss: 0.7648, iou: 0.1347, f1: 0.2352
@20230803_1928 @20230803_1928 [EP 8/500] lr: 1.5999999959603883e-05, took: 42.30s (avg: 42.30s), ETA: 0d:5h:44m:24s [TRAIN] loss: 0.7688, iou: 0.1338, f1: 0.2312 [EVAL] loss: 0.7216, iou: 0.1638, f1: 0.2784
@20230803_1928 @20230803_1928 [EP 9/500] lr: 1.8000000636675395e-05, took: 42.92s (avg: 42.92s), ETA: 0d:5h:43m:42s [TRAIN] loss: 0.7224, iou: 0.1650, f1: 0.2776 [EVAL] loss: 0.6616, iou: 0.2060, f1: 0.3384
@20230803_1929 @20230803_1929 [EP 10/500] lr: 1.9999999494757503e-05, took: 43.14s (avg: 43.14s), ETA: 0d:5h:51m:10s [TRAIN] loss: 0.6673, iou: 0.2041, f1: 0.3327 [EVAL] loss: 0.6104, iou: 0.2452, f1: 0.3896
@20230803_1930 @20230803_1930 [EP 11/500] lr: 2.2000000171829015e-05, took: 43.41s (avg: 43.41s), ETA: 0d:5h:50m:27s [TRAIN] loss: 0.6257, iou: 0.2350, f1: 0.3743 [EVAL] loss: 0.5780, iou: 0.2704, f1: 0.4220
@20230803_1931 @20230803_1931 [EP 12/500] lr: 2.4000000848900527e-05, took: 42.24s (avg: 42.24s), ETA: 0d:5h:41m:36s [TRAIN] loss: 0.5869, iou: 0.2652, f1: 0.4131 [EVAL] loss: 0.5436, iou: 0.2993, f1: 0.4564
@20230803_1931 @20230803_1931 [EP 13/500] lr: 2.5999999706982635e-05, took: 43.23s (avg: 43.23s), ETA: 0d:5h:49m:1s [TRAIN] loss: 0.5601, iou: 0.2872, f1: 0.4399 [EVAL] loss: 0.5155, iou: 0.3235, f1: 0.4845
@20230803_1932 @20230803_1932 [EP 14/500] lr: 2.8000000384054147e-05, took: 42.82s (avg: 42.82s), ETA: 0d:5h:40m:12s [TRAIN] loss: 0.5342, iou: 0.3088, f1: 0.4658 [EVAL] loss: 0.5073, iou: 0.3302, f1: 0.4927
@20230803_1933 @20230803_1933 [EP 15/500] lr: 3.000000106112566e-05, took: 42.35s (avg: 42.35s), ETA: 0d:5h:39m:30s [TRAIN] loss: 0.5162, iou: 0.3251, f1: 0.4838 [EVAL] loss: 0.4785, iou: 0.3567, f1: 0.5215
@20230803_1934 @20230803_1933 [EP 16/500] lr: 3.199999991920777e-05, took: 42.66s (avg: 42.66s), ETA: 0d:5h:38m:48s [TRAIN] loss: 0.4986, iou: 0.3404, f1: 0.5014 [EVAL] loss: 0.4630, iou: 0.3710, f1: 0.5370
@20230803_1934 @20230803_1934 [EP 17/500] lr: 3.400000059627928e-05, took: 42.82s (avg: 42.82s), ETA: 0d:5h:38m:6s [TRAIN] loss: 0.4757, iou: 0.3610, f1: 0.5243 [EVAL] loss: 0.4425, iou: 0.3907, f1: 0.5575
@20230803_1935 @20230803_1935 [EP 18/500] lr: 3.600000127335079e-05, took: 42.14s (avg: 42.14s), ETA: 0d:5h:37m:24s [TRAIN] loss: 0.4554, iou: 0.3805, f1: 0.5446 [EVAL] loss: 0.4248, iou: 0.4081, f1: 0.5752
