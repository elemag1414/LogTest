

** [alpha] Launch Training on 20230831_0528
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "weight_dice": 0.202208082367688,
  "alpha": 0.687479739608844,
  "gamma": 3.462163271866709,
  "early_stop": {
    "enabled": true,
    "patience": 30
  },
  "plateau": {
    "factor": 0.5,
    "patience": 10,
    "min_lr": 5e-06
  },
  "warmup_epochs": 15,
  "lr_scheduling": "exp",
  "lr": 8.67276e-05,
  "stepwise_scheduling": true,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "decay_per_step_factor": 3,
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230831_0530 @20230831_0530 [EP 1/500] lr: 8.223444456234574e-05, took: 115.63s (avg: 115.63s), ETA: 0d:15h:56m:25s [TRAIN] loss: 0.1474, iou: 0.1772, f1: 0.2761 [EVAL] loss: 0.2088, iou: 0.0019, f1: 0.0039
@20230831_0531 @20230831_0531 [EP 2/500] lr: 7.816557626938447e-05, took: 61.06s (avg: 61.06s), ETA: 0d:8h:26m:18s [TRAIN] loss: 0.0548, iou: 0.5847, f1: 0.7326 [EVAL] loss: 0.2145, iou: 0.0003, f1: 0.0006
@20230831_0532 @20230831_0532 [EP 3/500] lr: 7.448038377333432e-05, took: 61.10s (avg: 61.10s), ETA: 0d:8h:25m:17s [TRAIN] loss: 0.0332, iou: 0.7247, f1: 0.8391 [EVAL] loss: 0.2160, iou: 0.0001, f1: 0.0002
@20230831_0533 @20230831_0533 [EP 4/500] lr: 7.112701860023662e-05, took: 61.46s (avg: 61.46s), ETA: 0d:8h:24m:16s [TRAIN] loss: 0.0261, iou: 0.7774, f1: 0.8741 [EVAL] loss: 0.2133, iou: 0.0078, f1: 0.0150
@20230831_0534 @20230831_0534 [EP 5/500] lr: 6.80626108078286e-05, took: 61.22s (avg: 61.22s), ETA: 0d:8h:23m:15s [TRAIN] loss: 0.0232, iou: 0.8003, f1: 0.8885 [EVAL] loss: 0.1720, iou: 0.1166, f1: 0.2037
@20230831_0535 @20230831_0535 [EP 6/500] lr: 6.525134813273326e-05, took: 61.18s (avg: 61.18s), ETA: 0d:8h:22m:14s [TRAIN] loss: 0.0200, iou: 0.8251, f1: 0.9038 [EVAL] loss: 0.0826, iou: 0.4489, f1: 0.6168
@20230831_0536 @20230831_0536 [EP 7/500] lr: 6.266310083447024e-05, took: 61.23s (avg: 61.23s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0195, iou: 0.8294, f1: 0.9063 [EVAL] loss: 0.0650, iou: 0.5408, f1: 0.6992
@20230831_0537 @20230831_0537 [EP 8/500] lr: 6.027235576766543e-05, took: 61.32s (avg: 61.32s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0188, iou: 0.8354, f1: 0.9099 [EVAL] loss: 0.0524, iou: 0.6117, f1: 0.7573
@20230831_0538 @20230831_0538 [EP 9/500] lr: 5.8057328715221956e-05, took: 61.54s (avg: 61.54s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0178, iou: 0.8434, f1: 0.9147 [EVAL] loss: 0.0382, iou: 0.6995, f1: 0.8218
@20230831_0539 @20230831_0539 [EP 10/500] lr: 5.5999335017986596e-05, took: 61.19s (avg: 61.19s), ETA: 0d:8h:18m:10s [TRAIN] loss: 0.0172, iou: 0.8482, f1: 0.9176 [EVAL] loss: 0.0453, iou: 0.6561, f1: 0.7903
@20230831_0540 @20230831_0540 [EP 11/500] lr: 5.408225115388632e-05, took: 61.24s (avg: 61.24s), ETA: 0d:8h:17m:9s [TRAIN] loss: 0.0164, iou: 0.8547, f1: 0.9214 [EVAL] loss: 0.0496, iou: 0.6306, f1: 0.7713
@20230831_0541 @20230831_0541 [EP 12/500] lr: 5.229208545642905e-05, took: 61.11s (avg: 61.11s), ETA: 0d:8h:16m:8s [TRAIN] loss: 0.0157, iou: 0.8603, f1: 0.9247 [EVAL] loss: 0.0428, iou: 0.6721, f1: 0.8025
@20230831_0543 @20230831_0543 [EP 13/500] lr: 5.061662523075938e-05, took: 61.32s (avg: 61.32s), ETA: 0d:8h:15m:7s [TRAIN] loss: 0.0153, iou: 0.8639, f1: 0.9267 [EVAL] loss: 0.0317, iou: 0.7443, f1: 0.8527
@20230831_0544 @20230831_0544 [EP 14/500] lr: 4.904520028503612e-05, took: 61.48s (avg: 61.48s), ETA: 0d:8h:14m:6s [TRAIN] loss: 0.0152, iou: 0.8648, f1: 0.9273 [EVAL] loss: 0.0284, iou: 0.7674, f1: 0.8677
@20230831_0545 @20230831_0545 [EP 15/500] lr: 4.7568413720000535e-05, took: 61.05s (avg: 61.05s), ETA: 0d:8h:13m:5s [TRAIN] loss: 0.0148, iou: 0.8683, f1: 0.9293 [EVAL] loss: 0.0371, iou: 0.7089, f1: 0.8285
@20230831_0546 @20230831_0546 [EP 16/500] lr: 4.617795639205724e-05, took: 61.19s (avg: 61.19s), ETA: 0d:8h:12m:4s [TRAIN] loss: 0.0143, iou: 0.8727, f1: 0.9318 [EVAL] loss: 0.0401, iou: 0.6905, f1: 0.8158
@20230831_0547 @20230831_0547 [EP 17/500] lr: 4.4866479584015906e-05, took: 61.48s (avg: 61.48s), ETA: 0d:8h:11m:3s [TRAIN] loss: 0.0139, iou: 0.8754, f1: 0.9334 [EVAL] loss: 0.0314, iou: 0.7470, f1: 0.8544
@20230831_0548 @20230831_0548 [EP 18/500] lr: 4.3627438572002575e-05, took: 61.04s (avg: 61.04s), ETA: 0d:8h:10m:2s [TRAIN] loss: 0.0139, iou: 0.8759, f1: 0.9337 [EVAL] loss: 0.0289, iou: 0.7652, f1: 0.8664
@20230831_0549 @20230831_0549 [EP 19/500] lr: 4.245499076205306e-05, took: 61.17s (avg: 61.17s), ETA: 0d:8h:9m:1s [TRAIN] loss: 0.0139, iou: 0.8760, f1: 0.9337 [EVAL] loss: 0.0427, iou: 0.6737, f1: 0.8040
@20230831_0550 @20230831_0550 [EP 20/500] lr: 4.134392293053679e-05, took: 61.13s (avg: 61.13s), ETA: 0d:8h:8m:0s [TRAIN] loss: 0.0136, iou: 0.8782, f1: 0.9350 [EVAL] loss: 0.0407, iou: 0.6873, f1: 0.8136
@20230831_0551 @20230831_0551 [EP 21/500] lr: 4.028951298096217e-05, took: 61.04s (avg: 61.04s), ETA: 0d:8h:6m:59s [TRAIN] loss: 0.0132, iou: 0.8818, f1: 0.9370 [EVAL] loss: 0.0271, iou: 0.7777, f1: 0.8744
@20230831_0552 @20230831_0552 [EP 22/500] lr: 3.92875554098282e-05, took: 61.38s (avg: 61.38s), ETA: 0d:8h:5m:58s [TRAIN] loss: 0.0128, iou: 0.8854, f1: 0.9391 [EVAL] loss: 0.0324, iou: 0.7418, f1: 0.8509
@20230831_0553 @20230831_0553 [EP 23/500] lr: 3.833421942545101e-05, took: 61.17s (avg: 61.17s), ETA: 0d:8h:4m:57s [TRAIN] loss: 0.0127, iou: 0.8863, f1: 0.9396 [EVAL] loss: 0.0332, iou: 0.7364, f1: 0.8471
@20230831_0554 @20230831_0554 [EP 24/500] lr: 3.742605258594267e-05, took: 61.08s (avg: 61.08s), ETA: 0d:8h:3m:56s [TRAIN] loss: 0.0128, iou: 0.8854, f1: 0.9391 [EVAL] loss: 0.0229, iou: 0.8084, f1: 0.8935
@20230831_0555 @20230831_0555 [EP 25/500] lr: 3.655992622952908e-05, took: 61.17s (avg: 61.17s), ETA: 0d:8h:2m:55s [TRAIN] loss: 0.0128, iou: 0.8852, f1: 0.9390 [EVAL] loss: 0.0209, iou: 0.8231, f1: 0.9026
@20230831_0556 @20230831_0556 [EP 26/500] lr: 3.5732977266889066e-05, took: 61.94s (avg: 61.94s), ETA: 0d:8h:1m:54s [TRAIN] loss: 0.0127, iou: 0.8862, f1: 0.9395 [EVAL] loss: 0.0282, iou: 0.7713, f1: 0.8703
@20230831_0557 @20230831_0557 [EP 27/500] lr: 3.4942608181154355e-05, took: 61.20s (avg: 61.20s), ETA: 0d:8h:0m:53s [TRAIN] loss: 0.0125, iou: 0.8883, f1: 0.9407 [EVAL] loss: 0.0274, iou: 0.7775, f1: 0.8742
@20230831_0558 @20230831_0558 [EP 28/500] lr: 3.4186450648121536e-05, took: 61.52s (avg: 61.52s), ETA: 0d:7h:59m:52s [TRAIN] loss: 0.0121, iou: 0.8909, f1: 0.9422 [EVAL] loss: 0.0250, iou: 0.7938, f1: 0.8844
@20230831_0559 @20230831_0559 [EP 29/500] lr: 3.346232551848516e-05, took: 62.07s (avg: 62.07s), ETA: 0d:8h:6m:42s [TRAIN] loss: 0.0119, iou: 0.8932, f1: 0.9435 [EVAL] loss: 0.0220, iou: 0.8150, f1: 0.8975
@20230831_0600 @20230831_0600 [EP 30/500] lr: 3.276823917985894e-05, took: 61.13s (avg: 61.13s), ETA: 0d:7h:57m:50s [TRAIN] loss: 0.0117, iou: 0.8948, f1: 0.9443 [EVAL] loss: 0.0253, iou: 0.7919, f1: 0.8832
@20230831_0601 @20230831_0601 [EP 31/500] lr: 3.210236536688171e-05, took: 61.12s (avg: 61.12s), ETA: 0d:7h:56m:49s [TRAIN] loss: 0.0116, iou: 0.8955, f1: 0.9448 [EVAL] loss: 0.0296, iou: 0.7623, f1: 0.8644
@20230831_0602 @20230831_0602 [EP 32/500] lr: 3.146301241940819e-05, took: 61.41s (avg: 61.41s), ETA: 0d:7h:55m:48s [TRAIN] loss: 0.0114, iou: 0.8972, f1: 0.9457 [EVAL] loss: 0.0301, iou: 0.7590, f1: 0.8621
@20230831_0603 @20230831_0603 [EP 33/500] lr: 3.084862692048773e-05, took: 61.18s (avg: 61.18s), ETA: 0d:7h:54m:47s [TRAIN] loss: 0.0112, iou: 0.8989, f1: 0.9466 [EVAL] loss: 0.0273, iou: 0.7781, f1: 0.8745
@20230831_0604 @20230831_0604 [EP 34/500] lr: 3.0257777325459756e-05, took: 61.07s (avg: 61.07s), ETA: 0d:7h:53m:46s [TRAIN] loss: 0.0112, iou: 0.8993, f1: 0.9468 [EVAL] loss: 0.0231, iou: 0.8083, f1: 0.8935
@20230831_0606 @20230831_0606 [EP 35/500] lr: 2.9689137591049075e-05, took: 61.44s (avg: 61.44s), ETA: 0d:7h:52m:45s [TRAIN] loss: 0.0111, iou: 0.8998, f1: 0.9472 [EVAL] loss: 0.0238, iou: 0.8031, f1: 0.8902
@20230831_0607 @20230831_0607 [EP 36/500] lr: 2.914147262345068e-05, took: 61.10s (avg: 61.10s), ETA: 0d:7h:51m:44s [TRAIN] loss: 0.0110, iou: 0.9010, f1: 0.9478 [EVAL] loss: 0.0244, iou: 0.7988, f1: 0.8876
