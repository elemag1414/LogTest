

** [alpha] Launch Training on 20230822_2100
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "default",
  "early_stop": true,
  "patience": 30,
  "warmup_epochs": 15,
  "lr_scheduling": "const",
  "lr": 9e-05,
  "stepwise_scheduling": false,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230822_2102 @20230822_2102 [EP 1/500] lr: 9.000000136438757e-05, took: 116.44s (avg: 116.44s), ETA: 0d:16h:4m:44s [TRAIN] loss: 0.7248, iou: 0.1761, f1: 0.2752 [EVAL] loss: 0.9950, iou: 0.0025, f1: 0.0050
@20230822_2104 @20230822_2103 [EP 2/500] lr: 9.000000136438757e-05, took: 62.78s (avg: 62.78s), ETA: 0d:8h:34m:36s [TRAIN] loss: 0.2628, iou: 0.5902, f1: 0.7372 [EVAL] loss: 0.9995, iou: 0.0003, f1: 0.0005
@20230822_2105 @20230822_2105 [EP 3/500] lr: 9.000000136438757e-05, took: 61.72s (avg: 61.72s), ETA: 0d:8h:25m:17s [TRAIN] loss: 0.1533, iou: 0.7359, f1: 0.8467 [EVAL] loss: 0.9997, iou: 0.0001, f1: 0.0003
@20230822_2106 @20230822_2106 [EP 4/500] lr: 9.000000136438757e-05, took: 62.22s (avg: 62.22s), ETA: 0d:8h:32m:32s [TRAIN] loss: 0.1182, iou: 0.7894, f1: 0.8818 [EVAL] loss: 0.9885, iou: 0.0061, f1: 0.0115
@20230822_2107 @20230822_2107 [EP 5/500] lr: 9.000000136438757e-05, took: 62.42s (avg: 62.42s), ETA: 0d:8h:31m:30s [TRAIN] loss: 0.1057, iou: 0.8097, f1: 0.8943 [EVAL] loss: 0.7192, iou: 0.1669, f1: 0.2808
@20230822_2108 @20230822_2108 [EP 6/500] lr: 9.000000136438757e-05, took: 60.18s (avg: 60.18s), ETA: 0d:8h:14m:0s [TRAIN] loss: 0.0969, iou: 0.8239, f1: 0.9031 [EVAL] loss: 0.2824, iou: 0.5624, f1: 0.7176
@20230822_2110 @20230822_2110 [EP 7/500] lr: 9.000000136438757e-05, took: 58.33s (avg: 58.33s), ETA: 0d:7h:56m:34s [TRAIN] loss: 0.0939, iou: 0.8290, f1: 0.9061 [EVAL] loss: 0.2084, iou: 0.6571, f1: 0.7916
@20230822_2111 @20230822_2111 [EP 8/500] lr: 9.000000136438757e-05, took: 58.10s (avg: 58.10s), ETA: 0d:7h:55m:36s [TRAIN] loss: 0.0870, iou: 0.8405, f1: 0.9130 [EVAL] loss: 0.1556, iou: 0.7322, f1: 0.8444
@20230822_2112 @20230822_2112 [EP 9/500] lr: 9.000000136438757e-05, took: 57.81s (avg: 57.81s), ETA: 0d:7h:46m:27s [TRAIN] loss: 0.0854, iou: 0.8433, f1: 0.9146 [EVAL] loss: 0.1438, iou: 0.7498, f1: 0.8562
@20230822_2113 @20230822_2113 [EP 10/500] lr: 9.000000136438757e-05, took: 62.70s (avg: 62.70s), ETA: 0d:8h:26m:20s [TRAIN] loss: 0.0802, iou: 0.8520, f1: 0.9198 [EVAL] loss: 0.1331, iou: 0.7663, f1: 0.8669
@20230822_2115 @20230822_2115 [EP 11/500] lr: 9.000000136438757e-05, took: 62.45s (avg: 62.45s), ETA: 0d:8h:25m:18s [TRAIN] loss: 0.0755, iou: 0.8599, f1: 0.9245 [EVAL] loss: 0.1264, iou: 0.7768, f1: 0.8736
@20230822_2116 @20230822_2116 [EP 12/500] lr: 9.000000136438757e-05, took: 63.48s (avg: 63.48s), ETA: 0d:8h:32m:24s [TRAIN] loss: 0.0727, iou: 0.8647, f1: 0.9273 [EVAL] loss: 0.1444, iou: 0.7493, f1: 0.8556
@20230822_2117 @20230822_2117 [EP 13/500] lr: 9.000000136438757e-05, took: 63.28s (avg: 63.28s), ETA: 0d:8h:31m:21s [TRAIN] loss: 0.0746, iou: 0.8614, f1: 0.9254 [EVAL] loss: 0.1267, iou: 0.7763, f1: 0.8733
@20230822_2118 @20230822_2118 [EP 14/500] lr: 9.000000136438757e-05, took: 63.96s (avg: 63.96s), ETA: 0d:8h:30m:18s [TRAIN] loss: 0.0765, iou: 0.8583, f1: 0.9235 [EVAL] loss: 0.1549, iou: 0.7337, f1: 0.8451
@20230822_2119 @20230822_2119 [EP 15/500] lr: 9.000000136438757e-05, took: 63.20s (avg: 63.20s), ETA: 0d:8h:29m:15s [TRAIN] loss: 0.0775, iou: 0.8567, f1: 0.9225 [EVAL] loss: 0.1837, iou: 0.6919, f1: 0.8163
@20230822_2120 @20230822_2120 [EP 16/500] lr: 9.000000136438757e-05, took: 64.40s (avg: 64.40s), ETA: 0d:8h:36m:16s [TRAIN] loss: 0.0712, iou: 0.8674, f1: 0.9288 [EVAL] loss: 0.1406, iou: 0.7547, f1: 0.8594
