

** [alpha] Launch Training on 20230830_1251
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "weight_dice": 0.202208082367688,
  "alpha": 0.687479739608844,
  "gamma": 3.462163271866709,
  "early_stop": true,
  "patience": 30,
  "warmup_epochs": 15,
  "lr_scheduling": "const",
  "lr": 8.67276e-05,
  "stepwise_scheduling": false,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230830_1253 @20230830_1253 [EP 1/500] lr: 8.67276030476205e-05, took: 121.05s (avg: 121.05s), ETA: 0d:16h:46m:19s [TRAIN] loss: 0.1930, iou: 0.0300, f1: 0.0568 [EVAL] loss: 0.2046, iou: 0.0028, f1: 0.0056
@20230830_1254 @20230830_1254 [EP 2/500] lr: 8.67276030476205e-05, took: 71.58s (avg: 71.58s), ETA: 0d:9h:49m:18s [TRAIN] loss: 0.1153, iou: 0.2947, f1: 0.4339 [EVAL] loss: 0.2114, iou: 0.0012, f1: 0.0023
@20230830_1256 @20230830_1256 [EP 3/500] lr: 8.67276030476205e-05, took: 63.25s (avg: 63.25s), ETA: 0d:8h:41m:51s [TRAIN] loss: 0.0463, iou: 0.6364, f1: 0.7747 [EVAL] loss: 0.2145, iou: 0.0003, f1: 0.0006
@20230830_1257 @20230830_1257 [EP 4/500] lr: 8.67276030476205e-05, took: 61.31s (avg: 61.31s), ETA: 0d:8h:24m:16s [TRAIN] loss: 0.0301, iou: 0.7472, f1: 0.8543 [EVAL] loss: 0.2157, iou: 0.0009, f1: 0.0019
@20230830_1258 @20230830_1258 [EP 5/500] lr: 8.67276030476205e-05, took: 61.22s (avg: 61.22s), ETA: 0d:8h:23m:15s [TRAIN] loss: 0.0251, iou: 0.7851, f1: 0.8790 [EVAL] loss: 0.1657, iou: 0.1371, f1: 0.2341
@20230830_1259 @20230830_1259 [EP 6/500] lr: 8.67276030476205e-05, took: 61.44s (avg: 61.44s), ETA: 0d:8h:22m:14s [TRAIN] loss: 0.0223, iou: 0.8075, f1: 0.8931 [EVAL] loss: 0.0603, iou: 0.5653, f1: 0.7202
@20230830_1300 @20230830_1300 [EP 7/500] lr: 8.67276030476205e-05, took: 61.32s (avg: 61.32s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0202, iou: 0.8243, f1: 0.9033 [EVAL] loss: 0.0490, iou: 0.6320, f1: 0.7715
@20230830_1302 @20230830_1302 [EP 8/500] lr: 8.67276030476205e-05, took: 61.41s (avg: 61.41s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0195, iou: 0.8297, f1: 0.9065 [EVAL] loss: 0.0457, iou: 0.6526, f1: 0.7873
@20230830_1303 @20230830_1303 [EP 9/500] lr: 8.67276030476205e-05, took: 61.34s (avg: 61.34s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0191, iou: 0.8337, f1: 0.9089 [EVAL] loss: 0.0435, iou: 0.6818, f1: 0.7979
@20230830_1304 @20230830_1304 [EP 10/500] lr: 8.67276030476205e-05, took: 61.44s (avg: 61.44s), ETA: 0d:8h:18m:10s [TRAIN] loss: 0.0174, iou: 0.8471, f1: 0.9169 [EVAL] loss: 0.0391, iou: 0.6990, f1: 0.8181
@20230830_1306 @20230830_1306 [EP 11/500] lr: 8.67276030476205e-05, took: 61.78s (avg: 61.78s), ETA: 0d:8h:17m:9s [TRAIN] loss: 0.0169, iou: 0.8509, f1: 0.9192 [EVAL] loss: 0.0669, iou: 0.5470, f1: 0.6930
@20230830_1307 @20230830_1307 [EP 12/500] lr: 8.67276030476205e-05, took: 61.47s (avg: 61.47s), ETA: 0d:8h:16m:8s [TRAIN] loss: 0.0170, iou: 0.8502, f1: 0.9187 [EVAL] loss: 0.0322, iou: 0.7406, f1: 0.8501
@20230830_1308 @20230830_1308 [EP 13/500] lr: 8.67276030476205e-05, took: 61.43s (avg: 61.43s), ETA: 0d:8h:15m:7s [TRAIN] loss: 0.0161, iou: 0.8575, f1: 0.9230 [EVAL] loss: 0.0398, iou: 0.6922, f1: 0.8118
@20230830_1309 @20230830_1309 [EP 14/500] lr: 8.67276030476205e-05, took: 61.67s (avg: 61.67s), ETA: 0d:8h:14m:6s [TRAIN] loss: 0.0152, iou: 0.8648, f1: 0.9273 [EVAL] loss: 0.0276, iou: 0.7745, f1: 0.8721
@20230830_1310 @20230830_1310 [EP 15/500] lr: 8.67276030476205e-05, took: 61.43s (avg: 61.43s), ETA: 0d:8h:13m:5s [TRAIN] loss: 0.0150, iou: 0.8672, f1: 0.9287 [EVAL] loss: 0.0315, iou: 0.7471, f1: 0.8543
@20230830_1311 @20230830_1311 [EP 16/500] lr: 8.67276030476205e-05, took: 61.42s (avg: 61.42s), ETA: 0d:8h:12m:4s [TRAIN] loss: 0.0152, iou: 0.8654, f1: 0.9276 [EVAL] loss: 0.0393, iou: 0.6969, f1: 0.8202
@20230830_1312 @20230830_1312 [EP 17/500] lr: 8.67276030476205e-05, took: 61.52s (avg: 61.52s), ETA: 0d:8h:11m:3s [TRAIN] loss: 0.0156, iou: 0.8619, f1: 0.9256 [EVAL] loss: 0.0514, iou: 0.6227, f1: 0.7656
@20230830_1313 @20230830_1313 [EP 18/500] lr: 8.67276030476205e-05, took: 61.64s (avg: 61.64s), ETA: 0d:8h:10m:2s [TRAIN] loss: 0.0152, iou: 0.8650, f1: 0.9274 [EVAL] loss: 0.0286, iou: 0.7669, f1: 0.8671
@20230830_1314 @20230830_1314 [EP 19/500] lr: 8.67276030476205e-05, took: 61.40s (avg: 61.40s), ETA: 0d:8h:9m:1s [TRAIN] loss: 0.0146, iou: 0.8700, f1: 0.9303 [EVAL] loss: 0.0340, iou: 0.7307, f1: 0.8433
@20230830_1315 @20230830_1315 [EP 20/500] lr: 8.67276030476205e-05, took: 61.43s (avg: 61.43s), ETA: 0d:8h:8m:0s [TRAIN] loss: 0.0139, iou: 0.8761, f1: 0.9338 [EVAL] loss: 0.0277, iou: 0.7745, f1: 0.8721
@20230830_1317 @20230830_1317 [EP 21/500] lr: 8.67276030476205e-05, took: 61.52s (avg: 61.52s), ETA: 0d:8h:6m:59s [TRAIN] loss: 0.0133, iou: 0.8815, f1: 0.9369 [EVAL] loss: 0.0346, iou: 0.7274, f1: 0.8406
@20230830_1318 @20230830_1318 [EP 22/500] lr: 8.67276030476205e-05, took: 61.35s (avg: 61.35s), ETA: 0d:8h:5m:58s [TRAIN] loss: 0.0132, iou: 0.8820, f1: 0.9372 [EVAL] loss: 0.0315, iou: 0.7489, f1: 0.8552
@20230830_1319 @20230830_1319 [EP 23/500] lr: 8.67276030476205e-05, took: 61.42s (avg: 61.42s), ETA: 0d:8h:4m:57s [TRAIN] loss: 0.0132, iou: 0.8820, f1: 0.9372 [EVAL] loss: 0.0323, iou: 0.7432, f1: 0.8514
@20230830_1320 @20230830_1320 [EP 24/500] lr: 8.67276030476205e-05, took: 61.41s (avg: 61.41s), ETA: 0d:8h:3m:56s [TRAIN] loss: 0.0132, iou: 0.8826, f1: 0.9375 [EVAL] loss: 0.0273, iou: 0.7793, f1: 0.8752
@20230830_1321 @20230830_1321 [EP 25/500] lr: 8.67276030476205e-05, took: 61.63s (avg: 61.63s), ETA: 0d:8h:2m:55s [TRAIN] loss: 0.0131, iou: 0.8830, f1: 0.9377 [EVAL] loss: 0.0305, iou: 0.7556, f1: 0.8597
@20230830_1322 @20230830_1322 [EP 26/500] lr: 8.67276030476205e-05, took: 61.39s (avg: 61.39s), ETA: 0d:8h:1m:54s [TRAIN] loss: 0.0137, iou: 0.8781, f1: 0.9349 [EVAL] loss: 0.0208, iou: 0.8239, f1: 0.9030
@20230830_1324 @20230830_1324 [EP 27/500] lr: 8.67276030476205e-05, took: 61.41s (avg: 61.41s), ETA: 0d:8h:0m:53s [TRAIN] loss: 0.0137, iou: 0.8785, f1: 0.9352 [EVAL] loss: 0.0205, iou: 0.8270, f1: 0.9049
@20230830_1325 @20230830_1325 [EP 28/500] lr: 8.67276030476205e-05, took: 61.69s (avg: 61.69s), ETA: 0d:7h:59m:52s [TRAIN] loss: 0.0132, iou: 0.8823, f1: 0.9373 [EVAL] loss: 0.0247, iou: 0.7962, f1: 0.8859
@20230830_1326 @20230830_1326 [EP 29/500] lr: 8.67276030476205e-05, took: 61.31s (avg: 61.31s), ETA: 0d:7h:58m:51s [TRAIN] loss: 0.0128, iou: 0.8860, f1: 0.9394 [EVAL] loss: 0.0282, iou: 0.7718, f1: 0.8706
@20230830_1327 @20230830_1327 [EP 30/500] lr: 8.67276030476205e-05, took: 61.43s (avg: 61.43s), ETA: 0d:7h:57m:50s [TRAIN] loss: 0.0126, iou: 0.8873, f1: 0.9401 [EVAL] loss: 0.0292, iou: 0.7640, f1: 0.8652
