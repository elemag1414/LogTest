

** [alpha] Launch Training on 20230820_1544
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 10,
  "Flag": true,
  "p": 0.4,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "default",
  "early_stop": false,
  "lr_scheduling": "const",
  "lr": 6e-05,
  "stepwise_scheduling": false,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n*\n*      0811_1027: lambda_weight = .7\n*\n*      0811_16xx: equal loss dice_loss + focal_loss + mse\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "./Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230820_1546 @20230820_1546 [EP 1/500] lr: 5.999999848427251e-05, took: 117.11s (avg: 117.11s), ETA: 0d:16h:13m:3s [TRAIN] loss: 0.9322, iou: 0.0361, f1: 0.0678 [EVAL] loss: 0.9947, iou: 0.0027, f1: 0.0053
@20230820_1547 @20230820_1547 [EP 2/500] lr: 5.999999848427251e-05, took: 63.37s (avg: 63.37s), ETA: 0d:8h:42m:54s [TRAIN] loss: 0.6163, iou: 0.2496, f1: 0.3837 [EVAL] loss: 0.9961, iou: 0.0019, f1: 0.0039
@20230820_1548 @20230820_1548 [EP 3/500] lr: 5.999999848427251e-05, took: 65.42s (avg: 65.42s), ETA: 0d:8h:58m:25s [TRAIN] loss: 0.2834, iou: 0.5648, f1: 0.7166 [EVAL] loss: 0.9989, iou: 0.0006, f1: 0.0011
@20230820_1549 @20230820_1549 [EP 4/500] lr: 5.999999848427251e-05, took: 64.25s (avg: 64.25s), ETA: 0d:8h:49m:4s [TRAIN] loss: 0.1700, iou: 0.7115, f1: 0.8300 [EVAL] loss: 0.9983, iou: 0.0009, f1: 0.0017
@20230820_1550 @20230820_1550 [EP 5/500] lr: 5.999999848427251e-05, took: 64.98s (avg: 64.98s), ETA: 0d:8h:48m:0s [TRAIN] loss: 0.1311, iou: 0.7694, f1: 0.8689 [EVAL] loss: 0.8302, iou: 0.0957, f1: 0.1698
@20230820_1551 @20230820_1551 [EP 6/500] lr: 5.999999848427251e-05, took: 63.91s (avg: 63.91s), ETA: 0d:8h:38m:42s [TRAIN] loss: 0.1149, iou: 0.7949, f1: 0.8851 [EVAL] loss: 0.3579, iou: 0.4759, f1: 0.6421
@20230820_1552 @20230820_1552 [EP 7/500] lr: 5.999999848427251e-05, took: 63.27s (avg: 63.27s), ETA: 0d:8h:37m:39s [TRAIN] loss: 0.1041, iou: 0.8122, f1: 0.8959 [EVAL] loss: 0.2411, iou: 0.6134, f1: 0.7589
@20230820_1553 @20230820_1553 [EP 8/500] lr: 5.999999848427251e-05, took: 61.37s (avg: 61.37s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0911, iou: 0.8336, f1: 0.9089 [EVAL] loss: 0.1992, iou: 0.6700, f1: 0.8008
@20230820_1554 @20230820_1554 [EP 9/500] lr: 5.999999848427251e-05, took: 58.06s (avg: 58.06s), ETA: 0d:7h:54m:38s [TRAIN] loss: 0.0884, iou: 0.8380, f1: 0.9116 [EVAL] loss: 0.1413, iou: 0.7536, f1: 0.8587
@20230820_1555 @20230820_1555 [EP 10/500] lr: 5.999999848427251e-05, took: 58.07s (avg: 58.07s), ETA: 0d:7h:53m:40s [TRAIN] loss: 0.0906, iou: 0.8345, f1: 0.9094 [EVAL] loss: 0.2161, iou: 0.6469, f1: 0.7839
@20230820_1556 @20230820_1556 [EP 11/500] lr: 5.999999848427251e-05, took: 58.02s (avg: 58.02s), ETA: 0d:7h:52m:42s [TRAIN] loss: 0.0843, iou: 0.8450, f1: 0.9157 [EVAL] loss: 0.1592, iou: 0.7273, f1: 0.8408
@20230820_1557 @20230820_1557 [EP 12/500] lr: 5.999999848427251e-05, took: 58.07s (avg: 58.07s), ETA: 0d:7h:51m:44s [TRAIN] loss: 0.0788, iou: 0.8544, f1: 0.9212 [EVAL] loss: 0.1399, iou: 0.7560, f1: 0.8601
@20230820_1558 @20230820_1558 [EP 13/500] lr: 5.999999848427251e-05, took: 58.47s (avg: 58.47s), ETA: 0d:7h:50m:46s [TRAIN] loss: 0.0754, iou: 0.8601, f1: 0.9246 [EVAL] loss: 0.1429, iou: 0.7516, f1: 0.8571
@20230820_1559 @20230820_1559 [EP 14/500] lr: 5.999999848427251e-05, took: 57.98s (avg: 57.98s), ETA: 0d:7h:41m:42s [TRAIN] loss: 0.0791, iou: 0.8539, f1: 0.9209 [EVAL] loss: 0.1622, iou: 0.7232, f1: 0.8378
@20230820_1600 @20230820_1600 [EP 15/500] lr: 5.999999848427251e-05, took: 58.72s (avg: 58.72s), ETA: 0d:7h:48m:50s [TRAIN] loss: 0.0761, iou: 0.8590, f1: 0.9239 [EVAL] loss: 0.1987, iou: 0.6709, f1: 0.8013
@20230820_1601 @20230820_1601 [EP 16/500] lr: 5.999999848427251e-05, took: 58.38s (avg: 58.38s), ETA: 0d:7h:47m:52s [TRAIN] loss: 0.0724, iou: 0.8653, f1: 0.9276 [EVAL] loss: 0.1501, iou: 0.7404, f1: 0.8499
@20230820_1602 @20230820_1602 [EP 17/500] lr: 5.999999848427251e-05, took: 58.45s (avg: 58.45s), ETA: 0d:7h:46m:54s [TRAIN] loss: 0.0709, iou: 0.8679, f1: 0.9291 [EVAL] loss: 0.1382, iou: 0.7584, f1: 0.8618
@20230820_1603 @20230820_1603 [EP 18/500] lr: 5.999999848427251e-05, took: 58.36s (avg: 58.36s), ETA: 0d:7h:45m:56s [TRAIN] loss: 0.0708, iou: 0.8682, f1: 0.9292 [EVAL] loss: 0.1495, iou: 0.7415, f1: 0.8505
@20230820_1604 @20230820_1604 [EP 19/500] lr: 5.999999848427251e-05, took: 58.12s (avg: 58.12s), ETA: 0d:7h:44m:58s [TRAIN] loss: 0.0679, iou: 0.8732, f1: 0.9321 [EVAL] loss: 0.1940, iou: 0.6772, f1: 0.8060
@20230820_1605 @20230820_1605 [EP 20/500] lr: 5.999999848427251e-05, took: 58.30s (avg: 58.30s), ETA: 0d:7h:44m:0s [TRAIN] loss: 0.0662, iou: 0.8760, f1: 0.9338 [EVAL] loss: 0.1961, iou: 0.6743, f1: 0.8039
@20230820_1606 @20230820_1606 [EP 21/500] lr: 5.999999848427251e-05, took: 58.16s (avg: 58.16s), ETA: 0d:7h:43m:2s [TRAIN] loss: 0.0664, iou: 0.8757, f1: 0.9336 [EVAL] loss: 0.1770, iou: 0.7011, f1: 0.8230
@20230820_1607 @20230820_1607 [EP 22/500] lr: 5.999999848427251e-05, took: 58.11s (avg: 58.11s), ETA: 0d:7h:42m:4s [TRAIN] loss: 0.0651, iou: 0.8781, f1: 0.9349 [EVAL] loss: 0.1341, iou: 0.7648, f1: 0.8659
@20230820_1608 @20230820_1608 [EP 23/500] lr: 5.999999848427251e-05, took: 58.05s (avg: 58.05s), ETA: 0d:7h:41m:6s [TRAIN] loss: 0.0638, iou: 0.8804, f1: 0.9362 [EVAL] loss: 0.1194, iou: 0.7878, f1: 0.8806
@20230820_1609 @20230820_1609 [EP 24/500] lr: 5.999999848427251e-05, took: 58.05s (avg: 58.05s), ETA: 0d:7h:40m:8s [TRAIN] loss: 0.0651, iou: 0.8781, f1: 0.9349 [EVAL] loss: 0.1472, iou: 0.7455, f1: 0.8528
@20230820_1610 @20230820_1610 [EP 25/500] lr: 5.999999848427251e-05, took: 58.32s (avg: 58.32s), ETA: 0d:7h:39m:10s [TRAIN] loss: 0.0653, iou: 0.8777, f1: 0.9347 [EVAL] loss: 0.1707, iou: 0.7104, f1: 0.8293
@20230820_1611 @20230820_1611 [EP 26/500] lr: 5.999999848427251e-05, took: 58.01s (avg: 58.01s), ETA: 0d:7h:38m:12s [TRAIN] loss: 0.0636, iou: 0.8807, f1: 0.9364 [EVAL] loss: 0.2057, iou: 0.6614, f1: 0.7943
@20230820_1612 @20230820_1612 [EP 27/500] lr: 5.999999848427251e-05, took: 58.29s (avg: 58.29s), ETA: 0d:7h:37m:14s [TRAIN] loss: 0.0625, iou: 0.8826, f1: 0.9375 [EVAL] loss: 0.1496, iou: 0.7420, f1: 0.8504
@20230820_1613 @20230820_1613 [EP 28/500] lr: 5.999999848427251e-05, took: 58.45s (avg: 58.45s), ETA: 0d:7h:36m:16s [TRAIN] loss: 0.0618, iou: 0.8839, f1: 0.9382 [EVAL] loss: 0.1467, iou: 0.7456, f1: 0.8533
@20230820_1614 @20230820_1614 [EP 29/500] lr: 5.999999848427251e-05, took: 58.15s (avg: 58.15s), ETA: 0d:7h:35m:18s [TRAIN] loss: 0.0617, iou: 0.8841, f1: 0.9383 [EVAL] loss: 0.1204, iou: 0.7862, f1: 0.8796
@20230820_1615 @20230820_1615 [EP 30/500] lr: 5.999999848427251e-05, took: 58.39s (avg: 58.39s), ETA: 0d:7h:34m:20s [TRAIN] loss: 0.0622, iou: 0.8832, f1: 0.9378 [EVAL] loss: 0.1196, iou: 0.7875, f1: 0.8804
@20230820_1616 @20230820_1616 [EP 31/500] lr: 5.999999848427251e-05, took: 58.14s (avg: 58.14s), ETA: 0d:7h:33m:22s [TRAIN] loss: 0.0601, iou: 0.8869, f1: 0.9399 [EVAL] loss: 0.1151, iou: 0.7949, f1: 0.8849
@20230820_1617 @20230820_1617 [EP 32/500] lr: 5.999999848427251e-05, took: 58.18s (avg: 58.18s), ETA: 0d:7h:32m:24s [TRAIN] loss: 0.0580, iou: 0.8906, f1: 0.9420 [EVAL] loss: 0.1210, iou: 0.7854, f1: 0.8790
@20230820_1618 @20230820_1618 [EP 33/500] lr: 5.999999848427251e-05, took: 58.27s (avg: 58.27s), ETA: 0d:7h:31m:26s [TRAIN] loss: 0.0571, iou: 0.8922, f1: 0.9429 [EVAL] loss: 0.1146, iou: 0.7954, f1: 0.8854
@20230820_1619 @20230820_1619 [EP 34/500] lr: 5.999999848427251e-05, took: 58.22s (avg: 58.22s), ETA: 0d:7h:30m:28s [TRAIN] loss: 0.0572, iou: 0.8920, f1: 0.9428 [EVAL] loss: 0.1147, iou: 0.7952, f1: 0.8853
@20230820_1620 @20230820_1620 [EP 35/500] lr: 5.999999848427251e-05, took: 58.24s (avg: 58.24s), ETA: 0d:7h:29m:30s [TRAIN] loss: 0.0572, iou: 0.8921, f1: 0.9428 [EVAL] loss: 0.1756, iou: 0.7033, f1: 0.8244
@20230820_1621 @20230820_1621 [EP 36/500] lr: 5.999999848427251e-05, took: 58.53s (avg: 58.53s), ETA: 0d:7h:28m:32s [TRAIN] loss: 0.0571, iou: 0.8922, f1: 0.9429 [EVAL] loss: 0.1772, iou: 0.7008, f1: 0.8228
@20230820_1622 @20230820_1622 [EP 37/500] lr: 5.999999848427251e-05, took: 58.41s (avg: 58.41s), ETA: 0d:7h:27m:34s [TRAIN] loss: 0.0570, iou: 0.8923, f1: 0.9430 [EVAL] loss: 0.1517, iou: 0.7385, f1: 0.8483
@20230820_1623 @20230820_1623 [EP 38/500] lr: 5.999999848427251e-05, took: 58.48s (avg: 58.48s), ETA: 0d:7h:26m:36s [TRAIN] loss: 0.0562, iou: 0.8939, f1: 0.9438 [EVAL] loss: 0.1436, iou: 0.7505, f1: 0.8564
@20230820_1624 @20230820_1624 [EP 39/500] lr: 5.999999848427251e-05, took: 58.15s (avg: 58.15s), ETA: 0d:7h:25m:38s [TRAIN] loss: 0.0563, iou: 0.8937, f1: 0.9437 [EVAL] loss: 0.1265, iou: 0.7766, f1: 0.8735
@20230820_1625 @20230820_1625 [EP 40/500] lr: 5.999999848427251e-05, took: 58.22s (avg: 58.22s), ETA: 0d:7h:24m:40s [TRAIN] loss: 0.0574, iou: 0.8916, f1: 0.9426 [EVAL] loss: 0.1164, iou: 0.7927, f1: 0.8836
