

** [alpha] Launch Training on 20230804_1635
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "SEGFORMER",
  "baseModel": "MITB4",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 4,
  "early_stop": false,
  "patience": 50,
  "lr_scheduling": "wcos",
  "stepwise_scheduling": true,
  "lr": 0.0012,
  "optimizer": "AdamW",
  "decay_steps": 1.0,
  "warmup": 30,
  "train_annotation": "./Annotations/consolidate_train_db_20230711_1637.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230711_1637.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (0711_1637 version) - Updated DB \n* This consolidate DB is updated from 0703_2119 version \n* 4 GPUs are used to train models\n* SEGFORMER Model W/ MIT B2 Pretrained Wegiths\n* Optimize perf: AdamW is used. w/ lr=0.001\n",
  "numGPUs": 4,
  "isCompleted": false,
  "config": "Config/TRAIN_SEGFORMER_NEWDB_20230711_lr.yaml",
  "numTrainDB": 11530,
  "numEvalDB": 3524
}
@20230804_1645 @20230804_1645 [EP 1/500] lr: 3.9999998989515007e-05, took: 594.81s (avg: 594.81s), ETA: 3d:10h:20m:6s [TRAIN] loss: 0.8553, iou: 0.0851, f1: 0.1447 [EVAL] loss: 0.8313, iou: 0.0941, f1: 0.1687
@20230804_1650 @20230804_1650 [EP 2/500] lr: 7.999999797903001e-05, took: 306.30s (avg: 306.30s), ETA: 1d:18h:19m:48s [TRAIN] loss: 0.5996, iou: 0.2610, f1: 0.4004 [EVAL] loss: 0.5239, iou: 0.3225, f1: 0.4761
@20230804_1655 @20230804_1655 [EP 3/500] lr: 0.00012000000424450263, took: 304.04s (avg: 304.04s), ETA: 1d:17h:58m:8s [TRAIN] loss: 0.5049, iou: 0.3410, f1: 0.4951 [EVAL] loss: 0.4706, iou: 0.3721, f1: 0.5294
@20230804_1700 @20230804_1700 [EP 4/500] lr: 0.00015999999595806003, took: 304.24s (avg: 304.24s), ETA: 1d:17h:53m:4s [TRAIN] loss: 0.4514, iou: 0.3911, f1: 0.5486 [EVAL] loss: 0.4309, iou: 0.4105, f1: 0.5691
@20230804_1705 @20230804_1705 [EP 5/500] lr: 0.00020000000949949026, took: 273.26s (avg: 273.26s), ETA: 1d:13h:32m:15s [TRAIN] loss: 0.4007, iou: 0.4423, f1: 0.5993 [EVAL] loss: 0.3988, iou: 0.4432, f1: 0.6012
@20230804_1709 @20230804_1709 [EP 6/500] lr: 0.00024000000848900527, took: 262.87s (avg: 262.87s), ETA: 1d:11h:57m:8s [TRAIN] loss: 0.3615, iou: 0.4841, f1: 0.6385 [EVAL] loss: 0.3506, iou: 0.4948, f1: 0.6494
@20230804_1713 @20230804_1713 [EP 7/500] lr: 0.0002800000074785203, took: 263.10s (avg: 263.10s), ETA: 1d:12h:0m:59s [TRAIN] loss: 0.3427, iou: 0.5052, f1: 0.6574 [EVAL] loss: 0.3573, iou: 0.4876, f1: 0.6427
@20230804_1718 @20230804_1718 [EP 8/500] lr: 0.00031999999191612005, took: 262.55s (avg: 262.55s), ETA: 1d:11h:48m:24s [TRAIN] loss: 0.3253, iou: 0.5254, f1: 0.6747 [EVAL] loss: 0.3218, iou: 0.5262, f1: 0.6782
@20230804_1723 @20230804_1723 [EP 9/500] lr: 0.0003600000054575503, took: 288.93s (avg: 288.93s), ETA: 1d:15h:16m:48s [TRAIN] loss: 0.3110, iou: 0.5422, f1: 0.6890 [EVAL] loss: 0.3313, iou: 0.5163, f1: 0.6687
@20230804_1728 @20230804_1728 [EP 10/500] lr: 0.0004000000189989805, took: 305.93s (avg: 305.93s), ETA: 1d:17h:30m:50s [TRAIN] loss: 0.2957, iou: 0.5610, f1: 0.7043 [EVAL] loss: 0.2914, iou: 0.5625, f1: 0.7086
@20230804_1733 @20230804_1733 [EP 11/500] lr: 0.0004400000034365803, took: 304.03s (avg: 304.03s), ETA: 1d:17h:17m:36s [TRAIN] loss: 0.3151, iou: 0.5413, f1: 0.6849 [EVAL] loss: 0.3368, iou: 0.5101, f1: 0.6632
@20230804_1738 @20230804_1738 [EP 12/500] lr: 0.00048000001697801054, took: 304.32s (avg: 304.32s), ETA: 1d:17h:12m:32s [TRAIN] loss: 0.2905, iou: 0.5676, f1: 0.7095 [EVAL] loss: 0.2672, iou: 0.5927, f1: 0.7328
