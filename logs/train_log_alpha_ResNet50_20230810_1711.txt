

** [alpha] Launch Training on 20230810_1711
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "early_stop": false,
  "lr_scheduling": "exp",
  "lr": 0.0008,
  "stepwise_scheduling": true,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230810_1713 @20230810_1713 [EP 1/500] lr: 0.0007867820095270872, took: 114.79s (avg: 114.79s), ETA: 0d:15h:48m:6s [TRAIN] loss: 0.2679, iou: 0.5140, f1: 0.6315 [EVAL] loss: 0.7093, iou: 0.0001, f1: 0.0001
@20230810_1714 @20230810_1714 [EP 2/500] lr: 0.0007739338907413185, took: 61.54s (avg: 61.54s), ETA: 0d:8h:26m:18s [TRAIN] loss: 0.1156, iou: 0.7367, f1: 0.8468 [EVAL] loss: 0.7098, iou: 0.0000, f1: 0.0000
@20230810_1716 @20230810_1716 [EP 3/500] lr: 0.0007614986388944089, took: 61.65s (avg: 61.65s), ETA: 0d:8h:25m:17s [TRAIN] loss: 0.0947, iou: 0.7814, f1: 0.8764 [EVAL] loss: 0.7098, iou: 0.0000, f1: 0.0000
@20230810_1717 @20230810_1717 [EP 4/500] lr: 0.0007494565797969699, took: 61.89s (avg: 61.89s), ETA: 0d:8h:24m:16s [TRAIN] loss: 0.0874, iou: 0.7978, f1: 0.8868 [EVAL] loss: 0.6784, iou: 0.0249, f1: 0.0448
@20230810_1718 @20230810_1718 [EP 5/500] lr: 0.0007377894944511354, took: 61.62s (avg: 61.62s), ETA: 0d:8h:23m:15s [TRAIN] loss: 0.0797, iou: 0.8153, f1: 0.8977 [EVAL] loss: 0.4809, iou: 0.2004, f1: 0.3266
@20230810_1719 @20230810_1719 [EP 6/500] lr: 0.000726480211596936, took: 61.91s (avg: 61.91s), ETA: 0d:8h:22m:14s [TRAIN] loss: 0.0746, iou: 0.8275, f1: 0.9051 [EVAL] loss: 0.7720, iou: 0.0271, f1: 0.0523
@20230810_1720 @20230810_1720 [EP 7/500] lr: 0.0007155122584663332, took: 61.88s (avg: 61.88s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0809, iou: 0.8131, f1: 0.8962 [EVAL] loss: 0.0974, iou: 0.7755, f1: 0.8726
@20230810_1722 @20230810_1722 [EP 8/500] lr: 0.0007048706174828112, took: 61.74s (avg: 61.74s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0688, iou: 0.8410, f1: 0.9132 [EVAL] loss: 0.1248, iou: 0.7161, f1: 0.8333
