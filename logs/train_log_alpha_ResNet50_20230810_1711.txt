

** [alpha] Launch Training on 20230810_1711
{
  "HOST": "alpha",
  "SAVE": true,
  "github_log": true,
  "plot": true,
  "architecture": "DLV3+",
  "baseModel": "ResNet50",
  "gpuIDs": "0,1,2,3",
  "modelSize": [
    512,
    512
  ],
  "initialWeight": [
    null,
    null
  ],
  "num_epochs": 500,
  "resume": false,
  "prevEPOCH": 172,
  "resumeWeight": "./model_weights/ResNet50/20230605_1912_DLV3PP_ResNet50_BEST_AUG/variables/variables",
  "augmentation": true,
  "temperature_shift": 15,
  "Flag": true,
  "p": 0.2,
  "acc_thresh": 0.5,
  "batch_size": 14,
  "optimizer": "Adam",
  "loss": "combined_loss",
  "early_stop": false,
  "lr_scheduling": "exp",
  "lr": 0.0008,
  "stepwise_scheduling": true,
  "decay_steps": 1.0,
  "train_annotation": "./Annotations/consolidate_train_db_20230808_1942.txt",
  "eval_annotation": "./Annotations/consolidate_eval_db_20230808_1942.txt",
  "exp_comment": "* Updated Annotations are used\n* Updated consolidate DB used (20230808_1942 version) - Updated DB \n* 4 GPUs are used to train models\n* It's observed that initLr = 0.008 (stepwise) doesn't show good convergence. \n* So, new experiments on LR have been initiated. \n*   1st (0809_1052):    lr  0.004 : Doesn't seem to work\n*   2nd (0809_1202):    lr  0.001 \n* To improve the accuracy, we perform experiments on loss functions:\n*    We compare: dice loss only vs combined_loss (dice_loss (.7) + focal loss (.3) )\n*                For focal loss alpha of 0.25 and gamma of 2.0, lambda_weight of .3 are used \n*      lr (0810_1613) : 0.001\n*      lr (0810_1711): 0.0008\n",
  "numGPUs": 4,
  "finalEPOCH": 0,
  "isCompleted": false,
  "config": "Config/TRAIN_DLV3_ResNet50_NEWDB_20230808.yaml",
  "numTrainDB": 11830,
  "numEvalDB": 3600
}
@20230810_1713 @20230810_1713 [EP 1/500] lr: 0.0007867820095270872, took: 114.79s (avg: 114.79s), ETA: 0d:15h:48m:6s [TRAIN] loss: 0.2679, iou: 0.5140, f1: 0.6315 [EVAL] loss: 0.7093, iou: 0.0001, f1: 0.0001
@20230810_1714 @20230810_1714 [EP 2/500] lr: 0.0007739338907413185, took: 61.54s (avg: 61.54s), ETA: 0d:8h:26m:18s [TRAIN] loss: 0.1156, iou: 0.7367, f1: 0.8468 [EVAL] loss: 0.7098, iou: 0.0000, f1: 0.0000
@20230810_1716 @20230810_1716 [EP 3/500] lr: 0.0007614986388944089, took: 61.65s (avg: 61.65s), ETA: 0d:8h:25m:17s [TRAIN] loss: 0.0947, iou: 0.7814, f1: 0.8764 [EVAL] loss: 0.7098, iou: 0.0000, f1: 0.0000
@20230810_1717 @20230810_1717 [EP 4/500] lr: 0.0007494565797969699, took: 61.89s (avg: 61.89s), ETA: 0d:8h:24m:16s [TRAIN] loss: 0.0874, iou: 0.7978, f1: 0.8868 [EVAL] loss: 0.6784, iou: 0.0249, f1: 0.0448
@20230810_1718 @20230810_1718 [EP 5/500] lr: 0.0007377894944511354, took: 61.62s (avg: 61.62s), ETA: 0d:8h:23m:15s [TRAIN] loss: 0.0797, iou: 0.8153, f1: 0.8977 [EVAL] loss: 0.4809, iou: 0.2004, f1: 0.3266
@20230810_1719 @20230810_1719 [EP 6/500] lr: 0.000726480211596936, took: 61.91s (avg: 61.91s), ETA: 0d:8h:22m:14s [TRAIN] loss: 0.0746, iou: 0.8275, f1: 0.9051 [EVAL] loss: 0.7720, iou: 0.0271, f1: 0.0523
@20230810_1720 @20230810_1720 [EP 7/500] lr: 0.0007155122584663332, took: 61.88s (avg: 61.88s), ETA: 0d:8h:21m:13s [TRAIN] loss: 0.0809, iou: 0.8131, f1: 0.8962 [EVAL] loss: 0.0974, iou: 0.7755, f1: 0.8726
@20230810_1722 @20230810_1722 [EP 8/500] lr: 0.0007048706174828112, took: 61.74s (avg: 61.74s), ETA: 0d:8h:20m:12s [TRAIN] loss: 0.0688, iou: 0.8410, f1: 0.9132 [EVAL] loss: 0.1248, iou: 0.7161, f1: 0.8333
@20230810_1723 @20230810_1723 [EP 9/500] lr: 0.0006945409113541245, took: 61.65s (avg: 61.65s), ETA: 0d:8h:19m:11s [TRAIN] loss: 0.0651, iou: 0.8499, f1: 0.9185 [EVAL] loss: 0.1173, iou: 0.7315, f1: 0.8438
@20230810_1724 @20230810_1724 [EP 10/500] lr: 0.0006845095194876194, took: 61.73s (avg: 61.73s), ETA: 0d:8h:18m:10s [TRAIN] loss: 0.0642, iou: 0.8522, f1: 0.9198 [EVAL] loss: 0.2300, iou: 0.5234, f1: 0.6835
@20230810_1725 @20230810_1725 [EP 11/500] lr: 0.0006747638108208776, took: 61.66s (avg: 61.66s), ETA: 0d:8h:17m:9s [TRAIN] loss: 0.0625, iou: 0.8563, f1: 0.9223 [EVAL] loss: 0.1863, iou: 0.5979, f1: 0.7455
@20230810_1726 @20230810_1726 [EP 12/500] lr: 0.000665291678160429, took: 61.70s (avg: 61.70s), ETA: 0d:8h:16m:8s [TRAIN] loss: 0.0627, iou: 0.8559, f1: 0.9220 [EVAL] loss: 0.2277, iou: 0.5276, f1: 0.6867
@20230810_1727 @20230810_1727 [EP 13/500] lr: 0.0006560818874277174, took: 61.72s (avg: 61.72s), ETA: 0d:8h:15m:7s [TRAIN] loss: 0.0629, iou: 0.8552, f1: 0.9216 [EVAL] loss: 0.1153, iou: 0.7357, f1: 0.8466
@20230810_1728 @20230810_1728 [EP 14/500] lr: 0.0006471234955824912, took: 61.79s (avg: 61.79s), ETA: 0d:8h:14m:6s [TRAIN] loss: 0.0598, iou: 0.8629, f1: 0.9261 [EVAL] loss: 0.0972, iou: 0.7749, f1: 0.8724
@20230810_1729 @20230810_1729 [EP 15/500] lr: 0.0006384065491147339, took: 61.74s (avg: 61.74s), ETA: 0d:8h:13m:5s [TRAIN] loss: 0.0575, iou: 0.8685, f1: 0.9294 [EVAL] loss: 0.1601, iou: 0.6473, f1: 0.7829
@20230810_1730 @20230810_1730 [EP 16/500] lr: 0.0006299212691374123, took: 61.75s (avg: 61.75s), ETA: 0d:8h:12m:4s [TRAIN] loss: 0.0567, iou: 0.8704, f1: 0.9305 [EVAL] loss: 0.2273, iou: 0.5314, f1: 0.6873
@20230810_1731 @20230810_1731 [EP 17/500] lr: 0.0006216585752554238, took: 61.66s (avg: 61.66s), ETA: 0d:8h:11m:3s [TRAIN] loss: 0.0546, iou: 0.8755, f1: 0.9334 [EVAL] loss: 0.1122, iou: 0.7432, f1: 0.8511
@20230810_1732 @20230810_1732 [EP 18/500] lr: 0.0006136098527349532, took: 61.79s (avg: 61.79s), ETA: 0d:8h:10m:2s [TRAIN] loss: 0.0529, iou: 0.8798, f1: 0.9358 [EVAL] loss: 0.1466, iou: 0.6765, f1: 0.8022
@20230810_1733 @20230810_1733 [EP 19/500] lr: 0.0006057668360881507, took: 61.73s (avg: 61.73s), ETA: 0d:8h:9m:1s [TRAIN] loss: 0.0525, iou: 0.8809, f1: 0.9364 [EVAL] loss: 0.1517, iou: 0.6625, f1: 0.7951
@20230810_1734 @20230810_1734 [EP 20/500] lr: 0.0005981219001114368, took: 61.72s (avg: 61.72s), ETA: 0d:8h:8m:0s [TRAIN] loss: 0.0533, iou: 0.8789, f1: 0.9353 [EVAL] loss: 0.1506, iou: 0.6644, f1: 0.7964
@20230810_1735 @20230810_1735 [EP 21/500] lr: 0.0005906674196012318, took: 61.77s (avg: 61.77s), ETA: 0d:8h:6m:59s [TRAIN] loss: 0.0534, iou: 0.8785, f1: 0.9351 [EVAL] loss: 0.1214, iou: 0.7240, f1: 0.8381
@20230810_1736 @20230810_1736 [EP 22/500] lr: 0.0005833965260535479, took: 61.57s (avg: 61.57s), ETA: 0d:8h:5m:58s [TRAIN] loss: 0.0547, iou: 0.8755, f1: 0.9334 [EVAL] loss: 0.1000, iou: 0.7686, f1: 0.8683
@20230810_1737 @20230810_1737 [EP 23/500] lr: 0.0005763024091720581, took: 61.87s (avg: 61.87s), ETA: 0d:8h:4m:57s [TRAIN] loss: 0.0540, iou: 0.8771, f1: 0.9343 [EVAL] loss: 0.1528, iou: 0.6597, f1: 0.7933
@20230810_1738 @20230810_1738 [EP 24/500] lr: 0.0005693787825293839, took: 61.58s (avg: 61.58s), ETA: 0d:8h:3m:56s [TRAIN] loss: 0.0522, iou: 0.8817, f1: 0.9369 [EVAL] loss: 0.1218, iou: 0.7218, f1: 0.8373
@20230810_1739 @20230810_1739 [EP 25/500] lr: 0.0005626195343211293, took: 61.73s (avg: 61.73s), ETA: 0d:8h:2m:55s [TRAIN] loss: 0.0488, iou: 0.8902, f1: 0.9417 [EVAL] loss: 0.1278, iou: 0.7095, f1: 0.8289
@20230810_1741 @20230810_1741 [EP 26/500] lr: 0.000556018843781203, took: 61.82s (avg: 61.82s), ETA: 0d:8h:1m:54s [TRAIN] loss: 0.0476, iou: 0.8932, f1: 0.9434 [EVAL] loss: 0.1629, iou: 0.6406, f1: 0.7788
@20230810_1742 @20230810_1742 [EP 27/500] lr: 0.0005495712975971401, took: 61.61s (avg: 61.61s), ETA: 0d:8h:0m:53s [TRAIN] loss: 0.0477, iou: 0.8929, f1: 0.9432 [EVAL] loss: 0.1285, iou: 0.7086, f1: 0.8278
@20230810_1743 @20230810_1743 [EP 28/500] lr: 0.0005432715406641364, took: 61.66s (avg: 61.66s), ETA: 0d:7h:59m:52s [TRAIN] loss: 0.0483, iou: 0.8914, f1: 0.9424 [EVAL] loss: 0.1193, iou: 0.7274, f1: 0.8409
@20230810_1744 @20230810_1744 [EP 29/500] lr: 0.0005371146253310144, took: 61.68s (avg: 61.68s), ETA: 0d:7h:58m:51s [TRAIN] loss: 0.0506, iou: 0.8855, f1: 0.9391 [EVAL] loss: 0.0859, iou: 0.8006, f1: 0.8886
@20230810_1745 @20230810_1745 [EP 30/500] lr: 0.0005310956621542573, took: 61.72s (avg: 61.72s), ETA: 0d:7h:57m:50s [TRAIN] loss: 0.0502, iou: 0.8866, f1: 0.9397 [EVAL] loss: 0.0799, iou: 0.8143, f1: 0.8971
@20230810_1746 @20230810_1746 [EP 31/500] lr: 0.000525210052728653, took: 61.49s (avg: 61.49s), ETA: 0d:7h:56m:49s [TRAIN] loss: 0.0471, iou: 0.8944, f1: 0.9441 [EVAL] loss: 0.1083, iou: 0.7511, f1: 0.8566
@20230810_1747 @20230810_1747 [EP 32/500] lr: 0.0005194534896872938, took: 61.68s (avg: 61.68s), ETA: 0d:7h:55m:48s [TRAIN] loss: 0.0449, iou: 0.9000, f1: 0.9472 [EVAL] loss: 0.0781, iou: 0.8188, f1: 0.8997
@20230810_1749 @20230810_1749 [EP 33/500] lr: 0.000513821782078594, took: 61.65s (avg: 61.65s), ETA: 0d:7h:54m:47s [TRAIN] loss: 0.0436, iou: 0.9032, f1: 0.9490 [EVAL] loss: 0.1444, iou: 0.6770, f1: 0.8052
@20230810_1750 @20230810_1750 [EP 34/500] lr: 0.0005083108553662896, took: 61.53s (avg: 61.53s), ETA: 0d:7h:53m:46s [TRAIN] loss: 0.0443, iou: 0.9015, f1: 0.9481 [EVAL] loss: 0.0871, iou: 0.7980, f1: 0.8868
@20230810_1751 @20230810_1751 [EP 35/500] lr: 0.0005029168678447604, took: 61.72s (avg: 61.72s), ETA: 0d:7h:52m:45s [TRAIN] loss: 0.0445, iou: 0.9010, f1: 0.9478 [EVAL] loss: 0.1046, iou: 0.7589, f1: 0.8619
@20230810_1752 @20230810_1752 [EP 36/500] lr: 0.0004976362106390297, took: 61.57s (avg: 61.57s), ETA: 0d:7h:51m:44s [TRAIN] loss: 0.0438, iou: 0.9028, f1: 0.9488 [EVAL] loss: 0.0797, iou: 0.8152, f1: 0.8975
